[
  {
    "path": "posts/gordian-algorithm/",
    "title": "GORDIAN algorithm",
    "description": "Note for EDA placement algorithm - GORDIAN algorithm",
    "author": [
      {
        "name": "Bo-Wei Chen",
        "url": "https://BWbwchen.github.io/"
      }
    ],
    "date": "2022-03-23",
    "categories": [
      "EDA",
      "placement",
      "CAD"
    ],
    "contents": "\n\nContents\nMath\nAlgorithm\n\nUse analytical method and linear programming video\nMath\nGoal is to minimize the total square of wire length\nthe sum of square wire length between each moveable\nnodes(gates).\nthe sum of square wire length between moveable nodes(gates) and\nfixed nodes(pin).\n\nObjective function(minimize it)\nx axis : \\(\\frac{1}{2}x^TLx +\nd_x^Tx\\)\ny axis : \\(\\frac{1}{2}y^TLy +\nd_y^Ty\\)\n\nAlgorithm\nConstruct the k-clique graph for each net\nthe weight will be \\(\\frac{2}{k}\\)\n\nBuild the laplacian matrix and write down the objective\nfunction\niterative optimize it\nfor the first time\nminimize without any constraint\n\nfor the other time\nminimize with balance factor \\(\\alpha\\)\nminimize objective function above and constraint\ncenter of gravity constraint\nthe center of gravity of nodes in separated part should match the\ncenter of gravity constraint\n\n\n\n\n\n\n",
    "preview": "https://cdn.pixabay.com/photo/2018/03/13/22/53/puzzle-3223941_960_720.jpg",
    "last_modified": "2022-03-27T23:05:16+08:00",
    "input_file": {}
  },
  {
    "path": "posts/mincut-placement/",
    "title": "Mincut placement with propagation ",
    "description": "Note for EDA placement algorithm - Mincut placement with propagation",
    "author": [
      {
        "name": "Bo-Wei Chen",
        "url": "https://BWbwchen.github.io/"
      }
    ],
    "date": "2022-03-20",
    "categories": [
      "EDA",
      "placement",
      "CAD"
    ],
    "contents": "\n\nContents\nMincut placement\nTerminal propagation\n\nMincut placement\nRepeatedly split the circuit into sub-circuits by [[Partitioning]]\nalgorithm.\nUntil each partition only occupied by a single gates (or sometimes\nspecial sub-circuits).\nTerminal propagation\nDuring the first partition, there are some cases that two gate\n\\(x\\) and \\(y\\) is in the adjacent block.(wire length =\n1)\nBut at the second partition, we tried to partition the block of\n\\(x\\) and make \\(x\\) away from \\(y\\) (wire length = 2)\nWe need to connect the \\(x\\) and\n\\(y\\) to prevent the additional wire\nlength.\nhttps://i.imgur.com/1oIpQof.png\\(m\\) and \\(i\\) should be in the same partition and at\nthe left most side, otherwise we need more wire length.\n\n",
    "preview": "https://cdn.pixabay.com/photo/2018/03/13/22/53/puzzle-3223941_960_720.jpg",
    "last_modified": "2022-03-27T23:05:20+08:00",
    "input_file": {}
  },
  {
    "path": "posts/ilp-floorplanning-algorithm/",
    "title": "ILP floorplanning algorithm ",
    "description": "Note for EDA floorplanning algorithm - ILP floorplanning algorithm",
    "author": [
      {
        "name": "Bo-Wei Chen",
        "url": "https://BWbwchen.github.io/"
      }
    ],
    "date": "2022-03-17",
    "categories": [
      "EDA",
      "floorplanning",
      "CAD"
    ],
    "contents": "\n\nContents\nGoal\nIdea\nAlgorithm\nFor flexible module\nProblem\n\nGoal\nGiven some module with width and height, and some constraint(eg.\nrotation or not .etc), output the minimize(eg. area .etc)\nfloorplanning.\nSuitable for fixed and flexible module.\nIdea\nThere are so many constraint(eg. module size, rotation or not .etc)\nand one objective function(minimize floorplanning) to optimize.\nUse Integer Linear programming to solve it!!\nOnly consider the integer solution by branch and bound. video\n\nAlgorithm\nObjective function: minimize \\(y^*\\) , it is the width of total chip\nsize(square chip here).\nUse \\((x_{ij}, y_{ij})\\) pair to\ndefine the location relationship of module \\(i\\) and \\(j\\) .\nUse \\(z_i\\) to mark rotation or\nnot. \\(z_i = 0\\) mean we will NOT\nrotation the module \\(i\\) .\nNon-overlapping constraint\nWrite down each pair of module and avoid the overlapping. Below is\none of the example.\n\n\nVariable constraint\nSize of module should positive.\nrelationship variable should be binary.\n\n\nChip width constraint\n\n\nChip height constraint\n\n\nFor flexible module\nConstraint style is same as above.\nSince the width and height of flexible module can be changed, only\narea and aspect ratio is given, We want to obtain the\nLinear relationship between height and width.\nUse Taylor\nexpansion!\n\\(hw = S\\) , \\(S\\) is the area.\n\\(h = \\frac{S}{w} = \\frac{S}{w_{max}} +\n(w_{max} - w)\\frac{S}{w_{max}^2} + \\dots\\)\nUse more term can obtain more accuracy.\n\n\nProblem\nFor flexible module, we use taylor expansion to approximate the\nrelationship between width and height.\nSince it is approximate value, we will have some error value\nthere.\nSolution: Use more term to get high quality answer.\n\n",
    "preview": "https://cdn.pixabay.com/photo/2016/11/24/20/30/architecture-1857175_960_720.jpg",
    "last_modified": "2022-03-27T23:03:14+08:00",
    "input_file": {}
  },
  {
    "path": "posts/normalized-polish-expression/",
    "title": "Normalized Polish Expression ",
    "description": "Note for EDA floorplanning algorithm - Normalized Polish Expression",
    "author": [
      {
        "name": "Bo-Wei Chen",
        "url": "https://BWbwchen.github.io/"
      }
    ],
    "date": "2022-03-16",
    "categories": [
      "EDA",
      "floorplanning",
      "CAD"
    ],
    "contents": "\n\nContents\nGoal\nProperty of\nnormalized polish expression\nIdea\nAlgorithm\nProblem\n\nGoal\nGiven the polish expression of a slicing tree, output the minimum\nsize of floor planning.\n\\(PE = 25V 1H374V H6V 8V H\\)\n\nProperty of normalized\npolish expression\nEach block(operand) appears exactly once in the string. (eg. only\none 2 or 5 in the string)\nBalloting property\nThe number of operands is larger than the number of operators\n\nNormality property\nNo consecutive operators of the same type in the string\n1-1 correspondence to a slicing tree. (NPE can obtain an unique\nslicing tree)\n\nIdea\nUse Simulate Annealing algorithm\nWhat is the neighbor of the current polish expression?\n\\(Move_1\\) : Swap adjacent\noperands.\neg. 25V -> 52V or 374V -> 734V\n\n\\(Move_2\\) : Swap two operators.\neg. 25V1H -> 25H1V\n\n\\(Move_3\\) : Swap adjacent operand\nand operator.\neg. 6V8V -> V68V\nNOTICE!! need to keep the Balloting property and Normality property\n.\n\n\nAlgorithm\nRandom polish expression \\(PE_0\\)\nParameters for Simulate Annealing algorithm\n\\(T_0\\) and \\(T_{end}\\)\nCooling rate \\(\\alpha\\)\nNumber of move at each temperature \\(M_t\\)\n\n\\(Z\\) is the optimal answer that we\ncurrently have\nFor \\(i\\) in each temperature\nPerform one of the three type of move above \\(PE_i'\\)\n\\(C_i = A_i + \\lambda \\cdot W_i\\)\n\\(C_i\\) is the cost of \\(PE_i'\\)\n\\(A_i\\) is the area of \\(PE_i'\\)\n\\(\\lambda\\) is a\nhyperparameter\n\\(W_i\\) is the wirelength of \\(PE_i'\\)\n\n\\(\\Delta C = C_i - C_{i-1}\\)\nDecide whether to accept this proposal\nIf \\(\\Delta C > 0\\)\nAccept this change\n\nelse\nCalculate the probability \\(p = exp(\\Delta\nC / T_i)\\)\nIf \\(p > rand(0, 1)\\)\nAccept this change\n\nelse\nReject this change\n\n\n\nUpdate the temperature\n\\(T = \\alpha T\\)\n\n\nProblem\nHow to generate the initial slicing tree input?\n\n\n\n",
    "preview": "https://cdn.pixabay.com/photo/2016/11/24/20/30/architecture-1857175_960_720.jpg",
    "last_modified": "2022-03-27T23:02:34+08:00",
    "input_file": {}
  },
  {
    "path": "posts/stockmeyer-algorithm/",
    "title": "Stockmeyer algorithm ",
    "description": "Note for EDA floorplanning algorithm - Stockmeyer algorithm",
    "author": [
      {
        "name": "Bo-Wei Chen",
        "url": "https://BWbwchen.github.io/"
      }
    ],
    "date": "2022-03-15",
    "categories": [
      "EDA",
      "floorplanning",
      "CAD"
    ],
    "contents": "\n\nContents\nGoal\nIdea\nAlgorithm\nProblem\n\nGoal\nGiven a slicing tree, output the minimum size of floor\nplanning.\n\nIdea\nFor 2 child with dimension \\((h_l,\nw_l)\\) and \\((h_r, w_r)\\)\nFor vertical node(eg. 1, 5 in the figure above)\n\\(D = (width_1 + width_2, max(height_1,\nheight_2))\\)\n\nFor horizontal node(eg. 2, 8 in the figure above)\n\\(D = (max(width_1, width_2), height_1 +\nheight_2)\\)\n\nAlgorithm\nObjective\nLeft child with dimension \\((h_l,\nw_l)\\)\nRight child with dimension \\((h_r,\nw_r)\\)\nList \\(L = \\{(h_l, w_l), (w_l,\nh_l)\\}\\) sort by the first or second element.\nList \\(R = \\{(h_l, w_l), (w_l,\nh_l)\\}\\) sort by the first or second element.\n\nFor vertical node\n\\(L, R\\) sort by the first\nelement\nLike merge two sorted list\n\\(l = l_1, r = r_1\\)\nif \\(l.second > r.second\\) ,\nthen \\(l = l_{next}\\)\nif \\(l.second < r.second\\) ,\nthen \\(r = r_{next}\\)\nif \\(l.second = r.second\\) , then\n\\(l = l_{next}, r = r_{next}\\)\n\nIt will generate a dimension list \\(D\\) contain multiple dimension\ncombination.\nWith \\(D = (width_1 + width_2,\nmax(height_1, height_2))\\)\n\nFor horizontal node\nLike above, but with \\(D = (max(width_1,\nwidth_2), height_1 + height_2)\\)\nLike merge two sorted list\n\\(l = l_1, r = r_1\\)\nif \\(l.first > r.first\\) , then\n\\(l = l_{next}\\)\nif \\(l.first < r.first\\) , then\n\\(r = r_{next}\\)\nif \\(l.first = r.first\\) , then\n\\(l = l_{next}, r = r_{next}\\)\n\n\nNow we know the total floor planning area by bottom-up the slicing\ntree.\nTo get the location of each node in the slicing tree, traverse the\nslicing tree top-down.\nProblem\nHow to generate the initial slicing tree input?\n\n\n\n",
    "preview": "https://cdn.pixabay.com/photo/2016/11/24/20/30/architecture-1857175_960_720.jpg",
    "last_modified": "2022-03-27T23:02:58+08:00",
    "input_file": {}
  },
  {
    "path": "posts/fbb-algorithm/",
    "title": "FBB algorithm ",
    "description": "Note for EDA partitioning algorithm - FBB algorithm",
    "author": [
      {
        "name": "Bo-Wei Chen",
        "url": "https://BWbwchen.github.io/"
      }
    ],
    "date": "2022-03-14",
    "categories": [
      "EDA",
      "partitioning",
      "CAD"
    ],
    "contents": "\n\nContents\nGoal\nRe-construct the flow\nnetwork\nIteratively\ncall the maximum-flow algorithm\n\nGoal\nBalanced bipartition.\ndivided the circuit into 2 equal-sized partitions.\nRe-construct the flow\nnetwork\nTurn the original circuit into bi-directed graph\nhttps://i.imgur.com/xbvXKhs.pngfor each net \\(n\\) (hyperedge)\nadd vertices into graph \\(G\\)\nadd two auxiliary nodes \\(n_1\\) and\n\\(n_2\\) into the graph \\(G\\) with edge of capacity \\(0\\)\nconnect the vertices into \\(n_1\\)\nand \\(n_2\\) into all the vertices with\nedge of capacity \\(\\infty\\)\n\n\nIteratively call\nthe maximum-flow algorithm\nRandomly select 2 vertices as \\(s\\)\nand \\(t\\) for maximum-flow\nalgorithm.\nFound the augmenting path \\(AG\\)\n(maybe multiple paths)\nThere are some net \\(n\\) in the\n\\(AG\\) , check whether it will\nconstruct the balanced partition.\nYes\nWe found the partition with cut size equals to the maximum flow\nvalue.\n\nNo, We got a cut \\(C(X, X')\\)\nif \\(|X|\\) has less vertices, merge\nthe random vertices in \\(X'\\) into\nvertex \\(s\\) , vice versa.\n\n\n",
    "preview": "https://cdn.pixabay.com/photo/2017/07/13/19/30/texture-2501600_960_720.jpg",
    "last_modified": "2022-03-27T23:00:51+08:00",
    "input_file": {}
  },
  {
    "path": "posts/eig-algorithm/",
    "title": "EIG algorithm",
    "description": "Note for EDA partitioning algorithm - EIG algorithm",
    "author": [
      {
        "name": "Bo-Wei Chen",
        "url": "https://BWbwchen.github.io/"
      }
    ],
    "date": "2022-03-13",
    "categories": [
      "EDA",
      "partitioning",
      "CAD"
    ],
    "contents": "\n\nContents\nGoal\nLaplacian matrix \\(L\\) of graph \\(G\\)\n\\(\\lambda_2\\) optimization for symmetric\nmatrix\nAlgorithm\n\nGoal\nBalanced bipartition.\ndivided the circuit into 2 equal-sized partitions. # Graph in Linear\nalgebra\ngood\nexplanation\n\\(A\\) is adjacency matrix of\nundirected graph \\(G\\) .\n\\(x\\) is a vector and is a value of\neach node in \\(G\\) .\n\\(Ax = y\\) and what is the meaning\nof \\(y\\) ?\n\\(y_i = \\sum_{(i, j) \\in E(G)}\nx_j\\)\n\\(y_i\\) is the sum of the neighbor\nvalue of \\(x_i\\)\n\n\\(Ax = \\lambda x\\)\nSince \\(A\\) is symmetric matrix\n\\(\\rightarrow\\) eigenvector are real\nand orthogonal!\n\nLaplacian matrix \\(L\\) of graph \\(G\\)\n\\(D\\) is degree matrix.\n\\(D_{ii} =\\) degree of vertex \\(v_i\\) .\n\\(D_{ij} = 0, i \\neq j\\)\n\n\\(L = D-A\\)\n\\(L's\\) trivial eigenpair\n$ [1, , 1] ^T $\nwith \\(\\lambda = 0\\)\n\n\n\\(\\lambda_2\\) optimization for symmetric\nmatrix\n\\(\\lambda_2 = min_x\n\\frac{x^TLx}{x^Tx}\\)\nThe meaning of \\(x^TLx\\)\n\\(x^TLx = \\sum_{i, j = 1}^{n} L_{ij}x_jx_i\n= \\sum_{i, j = 1}^{n} (D_{ij} - A_{ij})x_jx_i =\\\\ \\sum_{i, j = 1}^{n}\nD_{ij}x_jx_i - \\sum_{i, j = 1}^{n} A_{ij}x_jx_i = \\\\ \\sum_{i= 1}^{n}\nD_{ii}x_i^2 - \\sum_{(i, j) \\in E(G)} 2x_jx_i = \\\\ \\sum_{(i, j) \\in E(G)}\nx_i^2 - 2x_jx_i + x_j^2 =\\\\ \\sum_{(i, j) \\in E(G)} (x_i -\nx_j)^2\\)\n\nProperty of \\(x\\)\n\\(x\\) is an unit vector \\(\\rightarrow \\sum_{i} x_i^2 = 1\\)\n\\(x\\) is orthogonal with 1st\neigenvector \\(\\rightarrow \\sum_i x_i =\n0\\)\n\nBack to \\(\\lambda_2 = min_x\n\\frac{x^TLx}{x^Tx} = min_x \\frac{\\sum_{(i, j) \\in E(G)} (x_i -\nx_j)^2}{\\sum_i x_i^2} =\\)\n\\(\\lambda_2 = min_x \\sum_{(i, j) \\in E(G)}\n(x_i - x_j)^2\\)\n\\(\\sum_{(i, j) \\in E(G)} (x_i -\nx_j)^2\\) means we don’t want to cross the group.\n\\(\\sum_i x_i = 0\\) means we need to\nbalance this two group.\n\n\nWe can use \\(0\\) or the median\nvalue of \\(x_i\\) to split the\ngroup.\nAlgorithm\nRe-calculate the graph\nfor each net \\(n\\) with size \\(k\\)\nconstruct the \\(k\\) -clique models\nwith weight of \\(\\frac{1}{k-1}\\)\nor we can use intersection graph.\n\n\nBuild the Laplacian matrix \\(L\\)\n\nCompute the second smallest eigenvalue and eigenvector \\(x\\) using [[Lanczos algorithm]].\n\nSort \\(x\\)\n\nUse sorted \\(x\\) to evaluate \\(n-1\\) partitioning solutions\n\\(((v_1), (v_2, \\dots, v_n))\\)\n\\(((v_1, v_2), (v_3, \\dots,\nv_n))\\)\nand so on\n\nCalculate each partition’s ratio cuts\nratio cut \\(= \\frac{c(X, Y)}{|X||Y|} =\n\\frac{\\text{cut size}}{\\text{size of X} \\times \\text{size of\nY}}\\)\n\nChoose partitioning with smallest ratio cut and balanced\npartition.\n\n\n\n\n",
    "preview": "https://cdn.pixabay.com/photo/2017/07/13/19/30/texture-2501600_960_720.jpg",
    "last_modified": "2022-03-27T23:00:36+08:00",
    "input_file": {}
  },
  {
    "path": "posts/fm-algorithm/",
    "title": "Fiduccia and Mattheyses algorithm (FM algorithm) ",
    "description": "Note for EDA partitioning algorithm -  Fiduccia and Mattheyses algorithm (FM algorithm)",
    "author": [
      {
        "name": "Bo-Wei Chen",
        "url": "https://BWbwchen.github.io/"
      }
    ],
    "date": "2022-03-10",
    "categories": [
      "EDA",
      "partitioning",
      "CAD"
    ],
    "contents": "\n\nContents\nGoal\nApplied directly on\nhypergraph\nBucket data structure\nIteratively\nmove(each iteration is called “pass”)\n\nGoal\nBalanced bipartition.\ndivided the circuit into 2 equal-sized partitions.\nApplied directly on\nhypergraph\nBucket data structure\nLike bucket sort, we first prepare some bucket to boost the sorting\nprocess.\nBucket size\nfrom \\(max(\\forall size(edge))\\) to\n\\(-max(\\forall size(edge))\\)\n\nIteratively\nmove(each iteration is called “pass”)\nRandomly bipartitioning the graph \\(G\\) with hyperedge\nDo procedure on the unmarked vertex until no\nunmarked exist\nFirst compute the gain value for all node.\n\\(x \\in P_1\\)\n\\(FS(x)\\) is the number of nets\nthat have \\(x\\) as the only cell in\n\\(P_1\\)\n\\(TE(x)\\) is the number of nets\nthat contain \\(x\\) and are entirely in\n\\(P_1\\)\n\\(gain(x) = FS(x) - TE(x)\\)\n\nmove the vertex \\(x\\) with maximum\nvalue of gain to the opposite under the area constraint, and then\nmark vertex \\(x\\)\nUse bucket to sort the maximum value.\narea constraint(after moving) : \\(|size(P_1) - size(P_2)| \\leq 1\\)\n\nre-compute the gain value of the unmarked neighbor\nvertex of \\(x\\) .\nRecord the \\(gain(x, y)\\) and the\ncurrent cut size\n\nIf the initial cut size has reduced during the current “pass”\nDo the other “pass” on the best solution\nOtherwise, terminate.\n\n",
    "preview": "https://cdn.pixabay.com/photo/2017/07/13/19/30/texture-2501600_960_720.jpg",
    "last_modified": "2022-03-27T23:00:25+08:00",
    "input_file": {}
  },
  {
    "path": "posts/kl-algorithm/",
    "title": "Kernighan and Lin algorithm (KL algorithm)",
    "description": "Note for EDA partitioning algorithm - Kernighan and Lin algorithm (KL algorithm)",
    "author": [
      {
        "name": "Bo-Wei Chen",
        "url": "https://BWbwchen.github.io/"
      }
    ],
    "date": "2022-03-03",
    "categories": [
      "EDA",
      "partitioning",
      "CAD"
    ],
    "contents": "\n\nContents\nGoal\nRewrite\nthe original circuit graph into edge-weighted undirected graph \\(G\\)\nIteratively\nswap(each iteration is called “pass”)\n\nGoal\nBalanced bipartition.\ndivided the circuit into 2 equal-sized partitions.\nRewrite\nthe original circuit graph into edge-weighted undirected graph \\(G\\)\nRewrite the original circuit graph with hyper-edge, and it can share\nthe same node.\nEdge weight \\(c(x, y)\\) = \\(\\frac{1}{|e| - 1}\\) , where \\(e\\) is a hyper-edge and \\(|e|\\) is the size of that hyper-edge.\n\nIteratively\nswap(each iteration is called “pass”)\nRandomly bipartitioning the graph \\(G\\) (above rewritten graph)\nDo procedure on the unmarked vertex until no\nunmarked exist\nCompute all the possible swap pairs on the unmarked\nvertex.\nfor vertex \\(x \\in P_1\\) , we\ndefine:\n\\(E_x = \\sum_{i \\in P_2} c(x, i)\\)\nThis is the cut size of outward edge.\n\\(I_x = \\sum_{i \\in P_1} c(x, i)\\)\nThis is the cut size of inward edge.\n\nfor vertex \\(x \\in P_1, y \\in P_2\\)\n, if we swap them, the decrease of the cut size between \\(P_1\\) and \\(P_2\\) is :\n\\(gain(x, y) = (E_x - I_x - c(x, y)) +\n(E_y - I_y - c(x, y))\\)\n\n\nFind the pair with maximum \\(gain(x,\ny)\\) , swap \\((x, y)\\) , and\nthen mark vertex \\(x\\)\nand \\(y\\) .\nRecord the \\(gain(x, y)\\) and the\ncurrent cut size\n\nIf the initial cut size has reduced during the current “pass”\nDo the other “pass” on the best solution\nOtherwise, terminate.\n\n",
    "preview": "https://cdn.pixabay.com/photo/2017/07/13/19/30/texture-2501600_960_720.jpg",
    "last_modified": "2022-03-27T22:59:47+08:00",
    "input_file": {}
  },
  {
    "path": "posts/flowmap-algorithm/",
    "title": "FlowMap algorithm ",
    "description": "Note for EDA cluster algorithm - FlowMap algorithm",
    "author": [
      {
        "name": "Bo-Wei Chen",
        "url": "https://BWbwchen.github.io/"
      }
    ],
    "date": "2022-03-01",
    "categories": [
      "EDA",
      "clustering",
      "CAD"
    ],
    "contents": "\n\nContents\nGoal\nAssumption\nUser constraint\nLabeling phase\nMapping phase\n\nGoal\nTranslate the boolean network to K-input lookup table. (with input\npin constraint)\nFor FPGA K-input lookup table.\nThe maximum delay from PI to PO is minimized.\nAssumption\nDAG\neach node has a 0 delay\ninter-cluster has unit delay\nintra-cluster has 0 delay\nUser constraint\npin limit\nLabeling phase\nGet a topological sort : \\(T\\)\n(without PI)\n\\(l\\) is the delay.\neach node has a 0 delay\n\nFor each ordered node \\(t\\) ,\nconstruct graph \\(N_t\\) , which\ncontains all of the parent of \\(t\\) .\nthen add a source node \\(S\\) to \\(N_t\\) and connect it to all PIs node.\nCompute \\(p = max(l(\\forall \\text{fan-in\nnode of }t))\\)\nConstruct \\(N_t '\\) , where\n\\(l(v) = p, v \\in N_t\\) , \\(v\\) will be collapsed into \\(t\\) (同樣的 p值則縮點)\nConstruct a flow-network \\(N_t''\\) and duplicate the none-\n\\(s\\) and none- \\(t\\) node -> \\((x, x'), e(x, x') = 1\\) , the other\nedge are \\(\\infty\\)\nFind the cut \\(C(X'',\n\\bar{X''})\\) with cut size(number of edge in this cut)\n\\(\\leq\\) pin constraint\nIf found\n\\(cluster(t)\\) or in the paper\n\\(\\overline{X_t} = \\text{collapsed node and t}\n\\in N_t'\\)\n\\(l(t) = p\\)\n\nIf not found\n\\(cluster(t)\\) or in the paper\n\\(\\overline{X_t} = t\\)\n\\(l(t) = p + 1\\)\n\n\n\\(l_v = max(l_1, l_2)\\) (maximum\ndelay of node \\(v\\) with\ncluster size limit)\n\nMapping phase\nTry to combine some cluster.\nPut all PO nodes in a set \\(L\\)\nAnswer clusters will be in \\(S\\)\nFor each remove node \\(v\\) from\n\\(L\\)\nPush \\(cluster(v)\\) in to \\(S\\)\n\\(L = L \\cup input \\text{-}\nnode(cluster(v))\\) , where the node in \\(input \\text{-} node(cluster(v))\\) have not\nformed the cluster yet also not PI nodes(not in the answer set \\(S\\) yet).\n\nUntil \\(L\\) empty.\n\n\n\n",
    "preview": "https://cdn.pixabay.com/photo/2014/07/08/10/47/team-386673_960_720.jpg",
    "last_modified": "2022-03-27T22:57:27+08:00",
    "input_file": {}
  },
  {
    "path": "posts/rajaraman-and-wong-algorithm/",
    "title": "Rajaraman and Wong algorithm",
    "description": "Note for EDA cluster algorithm - Rajaraman and Wong algorithm",
    "author": [
      {
        "name": "Bo-Wei Chen",
        "url": "https://BWbwchen.github.io/"
      }
    ],
    "date": "2022-02-28",
    "categories": [
      "EDA",
      "clustering",
      "CAD"
    ],
    "contents": "\n\nContents\nGoal\nAssumption\nUser constraint\nLabeling phase\nClustering phase\n\nGoal\nMaximum delay from PI(primary input) to PO(primary output) is\nminimized\nAssumption\nDAG\neach node has a unique delay\ninter-cluster has constant delay\nintra-cluster does not incur any delay\nUser constraint\nNode delay\nEdge delay\nCluster size limit\nLabeling phase\nCompute \\(\\Delta(x, v)\\) , which\nmeans the maximum delay from \\(x\\) to\n\\(v\\)\nGet a topological sort : \\(T\\)\n(without PI)\n\\(l\\) is the delay.\nPI node : 1\nnon-PI node initialize with 0\n\nFor each ordered node \\(v\\) ,\nconstruct graph \\(G_v\\) , which\ncontains all of the parent of \\(v\\)\nCompute \\(l_v(x)\\) , \\(x \\in G_v / {v}\\)\n\\(l_v(x) = l(x) + \\Delta(x, v)\\) (\nthe delay of path from PI to \\(x\\) then\nto \\(v\\) )\n\nSort \\(l_v(x)\\) in decreasing\norder: \\(S\\)\nPush the element one-by-one from \\(S\\) to \\(cluster(v)\\) with cluster size\nlimit\nComputer \\(l_1\\) (intra-cluster\ndelay) and \\(l_2\\) (inter-cluster\ndelay).\nIf \\(cluster(v)\\) contains any PI\nnodes\n\\(l_1 = max(l_v(\\forall PI \\text{ }\nnode))\\)\n\nIf size of \\(S\\) is bigger than\ncluster size limit\n\\(l_2 = max(l_v(\\forall x) + D), x \\in G_v\n/ v\\) , \\(D\\) is inter-cluster\ndelay.\n\n\n\\(l_v = max(l_1, l_2)\\) (maximum\ndelay of node \\(v\\) with\ncluster size limit)\n\nClustering phase\nTry to combine some cluster.\nPut all PO nodes in a set \\(L\\)\nAnswer clusters will be in \\(S\\)\nFor each remove node \\(v\\) from\n\\(L\\)\nPush \\(cluster(v)\\) in to \\(S\\)\n\\(L = L \\cup input \\text{-}\nnode(cluster(v))\\) , where the node in \\(input \\text{-} node(cluster(v))\\) have not\nformed the cluster yet(not in the answer set \\(S\\) yet).\n\nUntil \\(L\\) empty.\n\n\n\n",
    "preview": "https://cdn.pixabay.com/photo/2014/07/08/10/47/team-386673_960_720.jpg",
    "last_modified": "2022-03-27T22:55:04+08:00",
    "input_file": {}
  },
  {
    "path": "posts/raft/",
    "title": "Raft ",
    "description": "Paper note of Raft consensus algorithm",
    "author": [
      {
        "name": "Bo-Wei Chen",
        "url": "https://BWbwchen.github.io/"
      }
    ],
    "date": "2022-01-23",
    "categories": [
      "distributed system",
      "raft"
    ],
    "contents": "\n\nContents\nGoal\n5 Property\nConsensus Algorithm\nRaft Basics\nLeader election\nLog replication\nSafety\nDynamic\nmember in the cluster (configuration change mechanism)\nLog Compaction (by\nsnapshot)\nMajority Vote\nWith Application\nProblem\n\nGoal\nSolve split-brain by majority vote.\nLeader election\nLog replication\nSafety\nThe stronger degree of coherency\n5 Property\nElection safety: Only 1 leader.\nLeader Append-Only : leader only append new entry in its log. (No\ndelete its log).\nLog Matching : identify log entry by (term number, log index).\nLeader Completeness: if a log entry is committed in a given term,\nthen it will appear in the leader’s log.\nState Machine Safety: If a machine has an applied log entry\nat index \\(t\\) , then no other server\nwill have a applied different entry at index \\(t\\) .\nConsensus Algorithm\nKeeping the replicated log in replicated state machine\nconsistent.\nOnce commands are properly replicated, each server’s state machine\nprocess them and make each server identical\nRaft Basics\nTerms (Time, clock) When does the term end? #raft_problem\nA follower receives no communication over a period of\ntime(election timeout), and it begins an election to choose a\nnew leader.\n\n\nRaft divides time into terms.\nEach term begins with an election.\nAct as a logical clock to distinguish which server is newer.\nUse term number to detect inconsistencies.\n\nState How to know the total number of the cluster? #raft_problem\nBy configuration.\n\n\nRPCs Calls\nRequestVote\nAppendEntries\nreplicate log\nheartbeat message(append 0 entries)\n\n\nLog (Term number, Log index) can represent an unique log entry.\nA log entry is considered committed if it is stored\non majority of the servers (safe for that entry to be applied\nto the state machine).\nLeader will decides when to apply the log entry command.\n#raft_problem\n\n\nLeader election\nA follower receives no communication over a period of\ntime(election timeout), and it begins an election to choose a\nnew leader.\nheartbeat time << election timeout << infinity\nRaft paper use 10ms ~ 500ms.\n\nincrease its current term numbers and\nchange the state to the candidate.\n\nThere are 3 cases:\nIt wins the election\nit receives votes from a majority of the servers, which with\nthe same term, in the cluster.\nHow to vote? Vote for whom? → first-come-first-serve.\nIt became a leader and sends heartbeat messages to\nall of the other servers.\n\nanother server is a leader already.\nThe leader’s term >= the candidate’s term → candidate\nbecame a follower.\nThe leader’s term < the candidate’s term → candidate\nrejects the leader and continues the election.\n\nno winner(eg. split vote)\nretry with a randomized election timeout.\n\n\nThere are some restriction for leader election:\nLeader election restriction\n\nLog replication\nLeader receive the client request → append the command to leader’s\nlog → issue AppendEntries to all followers → leader\napplied the log and return.\nWhat if follower crash? #raft_problem\nleader retries the AppendEntries RPC\nindefinitely.\n\n\nConsistency issue\nproperties : (2 entries in different logs have :)\nthe same (term number, log index) → same entry command\n\nthe same (term number, log index) → all of the preceding\nentries are the same (consistency check)\n\n\nIf the follower receive a log entry which doesn’t have any matched\n(term number, log index) with follower’s log, refuse to update, drop\nit.\nLeader crash will lead to inconsistencies.\nCommitting entries from previous terms\n\nSolved by overwriting the followers’ logs with leader’s log.\nfind the latest log entry where leader and follower agree → delete\nall the log entry after that entry → leader send the remain part.\nleader will maintain a nextIndex for each follower. If the\nnextIndex is different, the RPC failed. → leader decrease the\nnextIndex and try to match.\nnextIndex = index of the next new log entry.\n(prevLogIndex, prevLogTerm)\n\nInitialize nextindex with leader’s next new log entry index.\n\n\n\n\nSafety\nEach state machine should execute exactly same commands in the same\norder.\nLeader election\nrestriction\nLeader for any given term contains the entire previous term\ncommitted log.\nEnsure this property from the moment of election.\n\nWhen election, candidate request a RequestVote RPC.\nthe voter will compare the (term number, index number)\nthe candidate is older than voter → deny.\nthe candidate is more up-to-date than voter → vote\nit.\n\nHow to compare? #raft_problem\nCompare term first, new is better, then compare log index, new is\nbetter.\n\n\n\nCommitting entries\nfrom previous terms\nWhat if the leader crashes during committing an\nentry? What should new leader do? How to determine commitment?\nA log entry is considered committed if it is stored\non majority of the servers (safe for that entry to be applied\nto the state machine).\nOnly term 1 was committed. (c) shouldn’t happen. \nOnly try to commit NOW new entry to the replicas, once we\ndone this, all prior (un-)committed entry will be automatically\ncommitted.\n\nFollower and candidate crashes\nSolved by overwriting the followers’ logs with leader’s log.\n\n\nDynamic\nmember in the cluster (configuration change mechanism)\nEach server has different timing to apply the new configuration.\nCan’t directly change, caused it will have 2 leader in some\ncases.\n\nTwo-phase approach\noriginal configuration → old and new configuration (joint consensus)\n→ new configuration\nIn the joint consensus phase, old and new configuration work\ntogether to serve Raft service.\n\nProblem\nNew servers need a long time to initialize.\nNew server will be in non-voting state until it caught up with the\nrest of the cluster. (by leader’s snapshot Log Compaction (by snapshot)\n)\n\nThe leader of joint consensus phase leader may not be part of the\nnew configuration.\nleader step down to follower state. Wait for a new election.\n\nRemoved server may disrupt the cluster by re-election.\nserver disregard(ignore) RequestVote RPC during the minimum election\ntimeout of hearing from the leader.\n\n\nLog Compaction (by snapshot)\n\nMajority Vote\nodd number of servers.\nIf you have \\(2x + 1\\) servers,\nthen there can tolerant at most \\(x\\)\nbroken servers in order to run normally.\n\nWith Application\nRecord the client request\n          Start(request) -> (log index, term numbers)\nProblem\nDoes the failure really frequently happen? #raft_problem\nFailure may not happened so frequently\nBut slow follower will happen frequently.\nCould we mix the slow follower and failure up? #raft_problem\n\n\n",
    "preview": "https://cdn.pixabay.com/photo/2019/11/19/07/18/network-4636686_960_720.jpg",
    "last_modified": "2022-03-27T23:07:42+08:00",
    "input_file": {}
  },
  {
    "path": "posts/vmware-ft-paper-note/",
    "title": "VMware FT paper note",
    "description": "Paper note of VMware FT",
    "author": [
      {
        "name": "Bo-Wei Chen",
        "url": "https://BWbwchen.github.io/"
      }
    ],
    "date": "2021-12-21",
    "categories": [
      "distributed system",
      "VMware FT"
    ],
    "contents": "\n\nContents\nVMware FT\nGoal\nConstraint\nReplication\nState Transfer\nReplicated State\nMachine\n\nVMWare FT\nNon-Deterministic\nEvents\nOutput Rule\nTest-and-Set\n\nVMware FT\nGoal\nUse Replication to achieve Fault-tolerance.\nConstraint\nCannot fix the software bug.\nReplication\nThey all copy their own state, but the definition of the state is\ndifferent.\nState Transfer\nPrimary backup its own state(memory,\ndata), and send them to the backup server. If the primary\nis fail-stop, we can use the backup server instead.\nReplicated State Machine\nPrimary backup its own state(command), and\nsend them to the backup server. If the primary is fail-stop, we can use\nthe backup server instead.\nVMWare FT\nIt replica all the things, including RAM, register…etc. But GFS will\nonly replica application-level data, such as chunks.\nUse disk server as the local disk.\nUse log channel for backup server to sync the primary log event.\n(eg, sync generating random numbers, etc.)\nNon-Deterministic Events\nThe time of the interrupt.\nNormally, the input in this system is a network package, while the\nDMA of the network card receives a package, it will copy the content\ninto memory, and trigger an interrupt, which could differ in time.\nSolution: Bounce Buffer. When a package is received. VMM will stop\nthe primary, and copy the package content into the primary memory and\ntrigger an interrupt of the primary’s network card and then memorize the\nid of the now instruction. It does something to the backup server and\nmakes an interrupt at the id of the instruction of the primary interrupt\ninstruction.\n\nWeird instructions\neg. use system time.\n\nMulti-process(didn’t be mentioned in this paper, this paper is only\nfor single-core processor)\nIt’s unpredictable for the order of the instruction execution on\nmulti-process.\n\nOutput Rule\nThe output of the primary will be sent by a simulated network card,\nand the output of the backup server will be discarded.\nWhat if the network between primary and backup server crash and the\nprimary dead? The values in backup and primary are different.\nWhen the backup server receives the primary log(input) first, then\nsend output to the outside client.\nBottleneck here, since the primary need to sync and wait for the\nbackup server.\n(What if) input into primary, but output from backup?\n\nWhat if the response had been sent to the client, but primary crash.\nBut the original request hasn’t been executed by the backup server?\nWhen the service switch to the backup server, it will wait until the\nbackup server consumes all the buffered request and have the same state\nas the original primary then it will start to take over the duty.\nDuplicate output?\nNope, since the output package will have the same information as the\nprimary output package, it will be filtered out at the TCP level.\n\n\nTest-and-Set\nWhat if the network between the primary and backup servers was\nbroken, but the primary and backup servers were all healthy? They all\nthink that the other is dead so it needs to take over for the duty.\nCall the third party service(test-and-set service) to decide,\nwhether use the primary or backup server.\nWhenever we need to change the primary, we need to connect with the\ntest-and-set service first to decide whether we could switch or not.\nIt’s like a lock.\n\n",
    "preview": "https://cdn.pixabay.com/photo/2019/11/19/07/18/network-4636686_960_720.jpg",
    "last_modified": "2022-03-27T23:08:06+08:00",
    "input_file": {}
  },
  {
    "path": "posts/google-file-system-paper-note/",
    "title": "Google File System paper note",
    "description": "Paper note of Google File System",
    "author": [
      {
        "name": "Bo-Wei Chen",
        "url": "https://BWbwchen.github.io/"
      }
    ],
    "date": "2021-12-20",
    "categories": [
      "distributed system",
      "gfs"
    ],
    "contents": "\n\nContents\nGFS\nGoal\nConstraint\nArchitecture\nMaster Node\nChunk Server\n\nTech\n讀\n寫\nGarbage Collection\nError Handle\nData Consistent\n\nProblem\n\nGFS\nGoal\n全局通用的儲存系統\n高容量高速→ 需要 sharding\n自動修復\n大量順序讀取，record append\nConstraint\n在一個 data center.\n可以允許一點點的錯誤\nonly guarantees that the data is written at least once as an atomic\nunit. → If writing success, data must have been written at the same\noffset on all replicas of some chunk.\nArchitecture\n單一 master node, 多個 chunk server\nMaster Node\n管理文件與 chunk info (file → chunk IDs)\nUse read-write lock → concurrent mutation in the same\ndirectory.\n\n\n儲存：（要存在硬碟，不用存在硬碟）\nfile → Chunk handlers\nChunk handlers → Chunk Data\n每個 chunk 在哪個 chunk server 上\nChunk version number\n主 chunk 在哪個 chunk server （因為寫入必須要在主 chunk）\n主 chunk expiration time.\n\n\n使用 log 紀錄每一次的操作\nChunk Server\n儲存實際 data\n64MB 為一個 chunk 並依照 chunk id 作為 filename 存成一個檔案\nTech\n讀\ngfs_readclient 發出請求讀 file, offset\nmaster 得知該 file 的 chunk id list，透過 offset / 64 得到對應的\nchunk id\nmaster 將該 chunk id 所在的 chunk server id list 與 chunk\nid(handle) 傳回給 client.\n\nclient 從 chunk server id list(cache this!) 挑一個獲取\ndata\nclient 將 chunk handle(id), byte range 交給 chunk server\nchunk server 將對應的 chunk id file with byte range 讀出交給\nclient\n寫\ngfs_write.pngRecord Append\n(追加在文件最後面)\n只能對 主 chunk (Primary Chunk) 來寫入\nBut, What if the primary chunk doesn’t exist?\nfind the up-to-date chunk replicas. (By the version number!!) → (Not\nfound? Crash)\nThe other will be the secondary chunk\nNotify the primary and secondary servers of their roles and new\nversion numbers. [The data in the primary and secondary servers are\nup-to-date??] [primary will notify the offset so if it\nis not up-to-date, it will leave a hole.]\nGiving the expiration time for the primary. (prevent multiple\nprimary servers, split-brain!!)\nIncrease the chunk version number\n\nIf the master wants to change the primary server, it can wait until\nthe primary is expired. (Not communication needed!)\nIf the writing data is too large, break it down into multiple write\noperations.\nClient 提供 filename 並要求追加內容→ master\nMaster → the server which contains the primary chunk (Primary\nserver)\nThe client sends the data to the primary and secondary servers, and\nthe server will save them in a temporary place. After the primary and\nsecondary server receives the data → “Receive the data!” to Client\nThe primary will check whether need a new chunk.\nOptimization: Send the data to the nearest server, and propagate it\nto the other server.\n\nClient “Append record now!” → Primary write the data and notify the\nsecondary server to write data.\nthe secondary will return the status to the primary server, “yes” or\n“no”.\nAll “yes” → “Done” to Client.\nelse → “Fail” to Client. The client should start from 1.\nBut!! The chunk server will Not recover the original data!\n\n\nGarbage Collection\nRename the filename into a hidden name with a deletion\ntimestamp.\nDuring the master’s regular scan, it removes the file and\nmetadata.\nError Handle\nData Consistent\n不同 chunk server 的 data 可能會略有不同！\nThe primary will notify the offset so if it is not up-to-date, it\nwill leave a hole.\nThe client needs to tolerant the out-of-order data order.\n\nIf you want to make it a\n強一致性 :\nDetect the repeated request. whether this request is retry.\nPrimary - secondary should be a two-phase commit.\nProblem\nOut of RAM of the master node.\nToo many requests for a single master node.\n\n\n\n",
    "preview": "https://cdn.pixabay.com/photo/2019/11/19/07/18/network-4636686_960_720.jpg",
    "last_modified": "2022-03-27T23:09:20+08:00",
    "input_file": {}
  },
  {
    "path": "posts/api-gateway-for-microservice-with-kong/",
    "title": "API gateway for microservice with Kong",
    "description": "API gateway with Kong",
    "author": [
      {
        "name": "Bo-Wei Chen",
        "url": "https://BWbwchen.github.io/"
      }
    ],
    "date": "2021-08-27",
    "categories": [
      "microservice"
    ],
    "contents": "\n\nContents\nIntroduction\nService discovery\nCode\n\nIntroduction\nI have implemented a JWT microservice. But I need an API gateway to\nredirect the outside HTTP request to the inner network. We need a server\nto receive an HTTP request and then redirect to the corresponding\nmicroservice. How do we know the actual IP address in our network? One\nis to hard-code the IP address in the API gateway. You can also use\nservice discovery to register your microservice IP address. So you can\ndynamically start service or shutdown service. In this tutorial, I use\nconsul as my service discovery registry.\nService discovery\nLet’s talk about service discovery first. There are 2 types of\nservice discovery. One is client-side, another is server-side.\nClient-side means you get the actual IP address of a microservice from\nthe service discovery registry first, then use that IP address to access\nthe microservice. Server-side means you send a request to the service\ndiscovery registry, and it will help you to query the corresponding\nmicroservice then send back the microservice response to you.\nCode\nI use consul as service registry and Kong as API gateway and use\ndocker-compose to set up my docker containers. ###\ndocker-compose.yml\nversion: \"3.5\"\nservices:\n  jwt_service:\n    container_name: jwt_service\n    build :\n      context : ./jwt\n      target: deploy \n    environment:\n      SECRETKEY: asdf\n      localIP: 192.168.18.5\n      PORT: 8087\n      consul_url: consul:8500\n    networks:\n      net:\n        ipv4_address: 192.168.18.5\n    expose :\n      - 8087\n    depends_on:\n      - consul\n\n  consul :\n    container_name : consul\n    image: consul\n    networks:\n      net:\n        ipv4_address: 192.168.18.4\n    expose:\n      - 8500\n      - 8600/udp\n    ports:\n      - \"8700:8500\"\n\n  kong:\n    image: kong:latest\n    volumes: \n      - ./kong.yml:/usr/local/kong/declarative/kong.yml\n    environment:\n      - KONG_DATABASE=off\n      - KONG_DECLARATIVE_CONFIG=/usr/local/kong/declarative/kong.yml\n      - KONG_PROXY_ACCESS_LOG=/dev/stdout\n      - KONG_ADMIN_ACCESS_LOG=/dev/stdout\n      - KONG_PROXY_ERROR_LOG=/dev/stderr\n      - KONG_ADMIN_ERROR_LOG=/dev/stderr\n      - KONG_ADMIN_LISTEN=0.0.0.0:8001, 0.0.0.0:8444 ssl\n      - KONG_DNS_RESOLVER=192.168.18.4:8600\n    ports:\n      - \"8000:8000\"\n      - \"8443:8443\"\n      - \"127.0.0.1:8001:8001\"\n      - \"127.0.0.1:8444:8444\"\n    networks:\n      - net\n\nnetworks:\n  net:\n    driver: bridge\n    ipam:\n      config:\n        - subnet: 192.168.18.0/24\n          gateway: 192.168.18.1\nWe need to edit the config file for the Kong API gateway. I set the\nDNS resolver to the consul. I use the docker bridge network to assign a\nstatic IP directly to each microservice container. If we didn’t assign a\nstatic IP, we use the service name to represent the IP address of each\ncontainer. When we access that IP, we will get the actual IP of that\ncontainer from docker. You can think that we use docker as a DNS\nresolver. But we use consul as DNS resolver! So it won’t know what is\nthe actual IP address of the service name. Make sure to assign a static\nIP to microservice! This is how I make it. If you have another good\nmethod to do it, please tell me!\nkong.yml\n_format_version: \"2.1\"\n\n_transform: false\n\nservices:\n  - name: jwt-service\n    url: http://jwt_service.service.dc1.consul\n    routes:\n      - name: jwt-route\n        paths: \n          - /jwt\nWe set the URL to the service name that we register to the\nconsul.\nThen I can access my microservice with path /jwt !\nIf you have any problems, feel free to ask me!\nYou can find me here 👉 tim.chenbw@gmail.com\nSubscribe to my substack here! 👉 subscribe\nme on substack !\n\n\n\n",
    "preview": "https://cdn.pixabay.com/photo/2015/10/31/11/58/call-center-1015274_960_720.jpg",
    "last_modified": "2022-03-27T23:10:23+08:00",
    "input_file": {}
  },
  {
    "path": "posts/tinyurl/",
    "title": "Tinyurl",
    "description": "Build a Full-Stack Tinyurl service in golang using Gin, Redis, MongoDB and Zookeeper",
    "author": [
      {
        "name": "Bo-Wei Chen",
        "url": "https://BWbwchen.github.io/"
      }
    ],
    "date": "2021-08-16",
    "categories": [
      "golang",
      "docker"
    ],
    "contents": "\n\nContents\nTinyurl\nArchitecture\nDeploy\nCI/CD (both frontend and\nbackend)\nProblem\nFuture\n\n\nTinyurl\nRecently, I learned the system design and tried my best to implement\na full-stack tinyurl service. Throughout this journey, I also learned\nhow to build a CI/CD pipeline (you can check this posts) and also learned how to use\nkubernetes.\nArchitecture\n\n\nI built this service on my own server, used nginx to serve static vue\nfrontend.\nI design this system aim to have 100 URLs requents per second, So the\nshorten name will have 7 characters and generated by use MD5.\nFirst, each api server got a range of number from Zookeeper, and then\nuse that number to generate the MD5 value, this method can guarantee\nthat the hash code won’t collision.\nSecond, take the first 7 characters as short name, you need to check\nwhether this short name is existing in the database. If existed in\ndatabase, just shift a character.\nThird, store into database and redis cache.\nFor get long URL, find in redis cache first. if not found, find in\nthe database.\nDeploy\nI deployed my service on rancher kubernetes 2.4.16 \nCI/CD (both frontend and\nbackend)\nI managed my code in self-hosted gitlab with gitlab-ci.\nbackend\n\nfrontend\n\nProblem\nAlthough I had checked the short name collision, I still got a\nlot of collision. Then I found it is the api server’s problem. A client\nsend request to api server, server will create a thread to handle that\nrequest, and use the counter number to compute the hash value. What if 2\nthread use counter value simultaneously ? They will use same value to\ncompute short name, and that short name doesn’t in database and they\nwill be store in database and redis cache!\nSo I make a mutex lock on counter variable to fix this bug.\nVue environment variable disappear when deploy on kubernetes.\nWhen I build the docker image of Vue frontend, I can’t build it with\nbackend way. In my api server, I can modify environment variable when I\ndeploy the service on Kubernetes. I can input the environment variable\nin deployment yaml file. But when I do the same thing to Vue frontend\ndocker image. I can not pass the environment variable via deployment\nyaml. At the end, I found that I should input the environment variable\nwhen I build the docker image.\nFuture\nI want to try to import microservice to this project, or future\nproject.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-03-27T23:11:42+08:00",
    "input_file": {}
  },
  {
    "path": "posts/epoll-study/",
    "title": "Epoll study ",
    "description": "let's study about epoll, blocking and non-blocking",
    "author": [
      {
        "name": "Bo-Wei Chen",
        "url": "https://BWbwchen.github.io/"
      }
    ],
    "date": "2021-08-13",
    "categories": [
      "epoll",
      "tcp"
    ],
    "contents": "\n\nContents\nBlocking and\nNon-Blocking\nSynchronous and\nAsynchronous\nNon-Blocking I/O in C\nReadiness of\nDescriptors\nThe bowels of epoll\n\n\nBlocking and Non-Blocking\nWhen you send a request to server. Server will prepare some data for\nyou (I/O operations). When you wait for data preparation, your program :\n1. hang up the thread until the server finished the preparation and send\nresponse to you. (Blocking) 2. will get response\nimmediately, but you probably got an error, if server hasn’t finished\nthe data preparation. Your program need to polling the server and\nchecking error code of each request. But you are not blocked !\n(Non-Blocking)\nSynchronous and Asynchronous\nWhen you send a request to server. Server will prepare some data for\nyou (I/O operations). When you wait for data preparation, your program :\n1. hand up the thread until the server finished the preparation and send\ntreponse to you. (Synchronous) 2. will get response\nimmediately, but server will run your request in “backgroung”. When\nserver finished your request, it will notify you, or run the\ncallback function. (Asynchronous)\nNon-Blocking I/O in C\nNormally, you can’t read or write directly to disk files. They\nusually do it via the kernel buffer cache as a proxy. But if you want to\nread or write directly, use O_SYNC flag when opening\nthe disk file.\nYou can put any file descriptor in the nonblocking mode, just setting\nthe no-delay flag O_NONBLOCK, which is an I/O operating\nmode, when opening the file.\nReadiness of Descriptors\nWe called it “ready”. When a file descriptor can perform an I/O\noperation without blocking, such as the arrival of new input or sock\nconnection establishment etc. There are 2 ways to find out the readiness\nof descriptors.\nLevel\nTriggered (select, poll, epoll) (when the condition is met)\nAt any time, we “try to”(or poll for) perform an I/O operation on an\nnon-blocking descriptor. If the I/O operation blocks, the system call\nreturns an error. If the I/O operation ready, we can actually perform\nthe entire I/O operation or just do nothing.\nEdge\nTriggered (Asynchronous) (when the status changed)\nWhen the I/O operation is ready, it will notify the process or “PUSH”\nto the process. The process can attempt to perform the maximum operation\nit possibly can every time it get a descriptor readiness notification,\nor it will need to wait until next notification arrival.\nBut even with both 2 non-blocking methods, an extremely large read or\nwrite call has the potential to block.\nSelect\nlevel triggered mechanism.\nint select(\n    int nfds, \n    fd_set *readfds, \n    fd_set *writefds,\n    fd_set *exceptfds, \n    struct timeval *timeout\n);\nSelect monitors 3 independent sets of descriptors. 1. readfds, check\nif a read will not block 2. writedfs, check if a write will not block 3.\nexceptfds, monitored for exceptional conditions.\nPoll\nBut what if you want not just read, write or exceptional conditions ?\nUse poll ! we pass in a set of descriptors each marked event that it\nneeds to track.\nint poll(struct pollfd *fds, nfds_t nfds, int timeout);\nstruct pollfd {\n    int   fd;         /* file descriptor */\n    short events;     /* requested events */\n    short revents;    /* returned events */\n};\nEpoll (event poll)\nEpoll instance is a kernel data structure. It allows for a process to\nmonotor multiplex I/O on mltiple descriptors. You have 3 system call to\ncontrol epoll instance :\nepoll_create()\n#include <sys/epoll.h>\n\nint epoll_create(int size);\nThe size augrument, which is the number of descriptors, is ignored\nsince Linux 2.6.8.\n#include <sys/epoll.h>\n\nint epoll_create1(int flags);\nflags can only be either 0 or EPOLL_COLEXEC. 1. flags is 0, it is the\nsame as epoll_create(). 1. flags is EPOLL_COLEXEC, it will set\nclose-on-exec flag on the new file descriptor.\nepoll_ctl()\n#include <sys/epoll.h>\n\nint epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);\nIt preformed op on target fd in\nepfd with event ##### op 1.\nEPOLL_CTL_ADD 2. EPOLL_CTL_MOD 3. EPOLL_CTL_DEL ##### events\ntypedef union epoll_data {\n    void        *ptr;\n    int          fd;\n    uint32_t     u32;\n    uint64_t     u64;\n} epoll_data_t;\n\nstruct epoll_event {\n    uint32_t     events;      /* Epoll events */\n    epoll_data_t data;        /* User data variable */\n};\nevents is just a bitmask that indicates which events\nfd is being monitored for. For example:\nEPOLLIN     : The associated file is available for read operations.\nEPOLLOUT    : The associated file is available for write operations.\nIf you want to perform multiple events, just OR-ing them. ####\nepoll_wait()\n#include <sys/epoll.h>\nint epoll_wait(int epfd, struct epoll_event *eventlist,\n                int maxevents, int timeout);\nblocks until any descriptor being monitored become ready for I/O\nThe bowels of epoll\nIn process\nLet’s first learn how the descriptor works. I create a file\ndescriptor in process A, I will get a file descriptor. The system will\nstore the descriptor and it’s file pointer in a table in my process A.\nEvery entry in this table has 2 fields : 1. descriptor flags. 2. file\npointer, point to an underlying kernel open file table. indexed by\ndescriptor.\nIn kernel\nEach file pointer point to an entry in kernel open file table. each\nentry in kernel open file table has 3 fields : 1. file offset 2. status\nflags 3. inode pointer.\nIn filesystem\nThen each inode pointer point to an entry in filesystem inode table.\neach entry in filesystem inode table has some fields : 1. file type. 2.\nfile locks. 3. etc.\nHow epoll works\nprocess A call epoll_create() to create an epoll\ninstance. The system will create an entry in kernel open file table, and\nit point to an entry in inode table. secondly, process A call\nepoll_ctl() to add a file descriptor fd0 to\nthe epoll instance’s interset list. The system will point the\nfd0 in epoll instance to fd0 in kernel open\nfile table (NOTICE: the kernel open file table entry is shared by\nreferenced descriptor !!) #### example Consider the situation above,\nprocess A fork process B without close-on-exec, process B will have\ntable as same as process B. for example, process A have fd0 and fork\nprocess B, it have a descriptor fd1. But fd0 and fd1 point to the same\nunderlying kernel open file table entry.\nFork epoll\nSo same case, but the fd0 and fd1 in the above case is epoll\ndescriptor. process A call epoll_wait(), after a moment it\ngot notifications. But at the same time, process B will also got the\nnotifications !!!\nSo we can say once a file descriptor is registered by a process with\nthe epoll instance, it will continue getting notifications about events\non the descriptor even if it closes the descriptors as long as the\nunderlying open file descriptor is still referenced by at least one\ndescriptor.\nPerformance\nSelect and poll are O(N) (or O(number of descriptors being\nmonitored)), but epoll is O(1) (or O(number of events that have\noccurred)). Because every time the descriptor become ready, kernel will\nadd it into the ready list in epoll instance. Once process call\nepoll_wait(), just return the ready list.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-03-25T08:55:57+08:00",
    "input_file": {}
  },
  {
    "path": "posts/moscow-notation/",
    "title": "Moscow notation",
    "description": "What is MOSCOW notation?",
    "author": [
      {
        "name": "Bo-Wei Chen",
        "url": "https://BWbwchen.github.io/"
      }
    ],
    "date": "2021-08-12",
    "categories": [
      "management"
    ],
    "contents": "\n\nContents\nMotivation\nMeaning\nExample\n\nMotivation\nDuring the internship at Logitech, I refactor and optimize the\ninternal test tool. When I finished the coding, my manager ask me to\nwrite a detailed document to record the core logic in this test\ntool.\nWhen I wrote the document, my manager ask me to use\nMOSCOW notation to list down the requirements of this\ntest tool.\nBut, What is MOSCOW notation ? 🤔\nMeaning\nMoSCoW is an acronym derived from four letters of each prioritization\n: M for MUST HAVE, S for SHOULD HAVE, C for COULD HAVE and W for WON’T\nHAVE. It didn’t have O, just for word pronounceable.\nExample\nSo when I write a test tool(prime detector) to detect prime number\nand composite number and we plan to detect floating point in the future,\nI will need to write the below requirement.\nPRIMEDETECTOR-1: MUST detect the prime number.\nPRIMEDETECTOR-2: MUST detect the composite number.\nPRIMEDETECTOR-3: SHOULD detect the floating point.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-03-25T08:55:57+08:00",
    "input_file": {}
  },
  {
    "path": "posts/cicd-on-gitlab-ci/",
    "title": "CI/CD on gitlab-ci",
    "description": "CI/CD on gitlab-ci with kubernetes",
    "author": [
      {
        "name": "Bo-Wei Chen",
        "url": "https://BWbwchen.github.io/"
      }
    ],
    "date": "2021-08-08",
    "categories": [
      "CI/CD",
      "docker",
      "docker-compose",
      "kubernetes"
    ],
    "contents": "\n\nContents\nEnvironment\ntinyURL\nTest on Local\nTest\nBuild\nDeploy\nFinal\n\nRecently, I have been writting a tinyURL service with a CI/CD\npipeline on self-hosted gitlab. I want to record this horrible and\ntime-consuming self-taught journey.\nOur CI/CD pipeline will be composed with test, build and deploy. We\nfirst test the code, check whether there is any mistake in our code\nlogic. Second, we build it to binary. Finally, we deploy our binary on\nthe cloud or your own kubernetes.\nEnvironment\ngolang 1.16.5\ndocker 20.10.7\ndocker-compose 1.29.2\nrancher 2.4.16\ngitlab-runner with docker executor\ntinyURL\ntinyURL was written for self-taught with backend developement,\nkubernetes, docker and DevOps. User will give the service a longURL.\nService will give back the short name of that URL. Once user query the\nservice with the short name, service will need to give back the original\nlong URL. for more info, check this\nThis is the overview architecture: \nTest on Local\nNotice that the whole system comprises 4 main module: api server,\nredis service, mongodb service and Zookeeper service. So I choose docker-compose\nto build a testing environment. Let’s check the setting yaml file:\nversion: \"3\"\nservices:\n    server:\n      container_name: myshorturl\n      image: shorturl_test\n      ports:\n        - 8081:8081\n      environment:\n        REDIS_URL: redis:6379\n        DB_URL: db:27017\n        ZOOKEEPER_URL: zookeeper:2181\n        PORT: 8081\n      depends_on:\n        - redis\n        - db\n        - zookeeper\n    redis:\n      container_name: redis\n      image: redis:alpine\n      expose:\n        - 6379\n    db:\n      container_name: monogodb\n      image: mongo:4.0.26-xenial\n      expose:\n        - 27017\n    zookeeper:\n      container_name: zookeeper\n      image: zookeeper:3.7.0\n      expose:\n        - 2181\nThere are 3 things to notice: 1. Firstly, Make sure the version\nnumber is 3, or it will fail in the testing pipeline. 2. Secondly, if\napi server want to connect with other service, connect the service name\ndirectly. eg: How to connect db ? just\n<dbprotocol>://db, such like : postgresql://db 3.\nLastly, the difference between ports and expose. - Ports will expose the\nport number and also publish them to the host machine.\n- Expose will expose the port between the containers,\nit will not publish the port to the host machine.\nThen run with command (it will shutdown containers automatically, if\napi-server finished test):\n$ docker-compose up --abort-on-container-exit --exit-code-from server\nTest\ntest-job:\n  stage: test\n  before_script:\n    - apk add docker-compose\n    - docker build -f Dockerfile-ci --tag shorturl_test .\n  script:\n    - docker-compose up --abort-on-container-exit --exit-code-from server\nSince we use gitlab-runner docker executor with\ndocker:stable image as default image, it only\nprovide docker command. You can check the official\ndocumentation. We need to install the docker-compose command for\ntesting. use apk to install it. Then build a docker image for golang\ntesting command.\n# Dockerfile-ci\nFROM golang:1.16\nWORKDIR /app\nCOPY . .\n\nCMD [ \"go\", \"test\", \"-v\" \nBuild\nbuild-job:  \n  image: \n    name: gcr.io/kaniko-project/executor:debug\n    entrypoint: [\"\"]\n  stage: build\n  script:\n     - mkdir -p /kaniko/.docker\n     - echo \"{\\\"auths\\\":{\\\"$DOCKER_REGISTRY\\\":{\\\"username\\\":\\\"${DOCKER_REGISTRY_USER}\\\",\\\"password\\\":\\\"${DOCKER_REGISTRY_PASSWORD}\\\"}}}\" > /kaniko/.docker/config.json\n     - /kaniko/executor --context $CI_PROJECT_DIR --dockerfile $CI_PROJECT_DIR/Dockerfile --destination $DOCKER_IMAGE\nWe use pre-build image to handle this stage.\nDeploy\nNow, We will try to deploy our service on kubernetes. I use rancher\nfor kubernetes.\nThere are some things need to sure. 1. kubeconfig file. 2. docker\nregistry username, password\nFirst, deploy a deployment on kubernetes. So we can automatically\ndeploy the next job trigger.\ndeploy-job:\n  stage: deploy\n  image: dtzar/helm-kubectl\n  before_script:\n    - sed -ie \"s/deploy-date-value/$(date)/g\" kubernetes/deploy.yaml\n    - mkdir -p /root/.kube/ && touch /root/.kube/config\n    - echo ${KUBERNETES_KUBE_CONFIG} | base64 -d > ${KUBECONFIG}\n  script:\n    - kubectl apply -f kubernetes/deploy.yaml\nI use deply-date-value as a trigger to trigger the deployment\nupdate.\nFinal\nThe CI/CD pipeline should run successfully ! Hooray ! 🎉\nIf you have other problem, feel free to ask me !\nYou can find me here 👉 tim.chenbw@gmail.com\n\n\n\n",
    "preview": {},
    "last_modified": "2022-03-25T08:55:57+08:00",
    "input_file": {}
  },
  {
    "path": "posts/go-interface-for-inheritance/",
    "title": "Go interface for inheritance",
    "description": "CRUD with clean code by using Go embed struct",
    "author": [
      {
        "name": "Bo-Wei Chen",
        "url": "https://BWbwchen.github.io/"
      }
    ],
    "date": "2021-08-07",
    "categories": [
      "golang"
    ],
    "contents": "\nLet’s check this example :\ntype SQL struct {\n    table SQL.table\n}\nWe want CRUD (create, read, update, delete) operation. So you might\nwrite the code like this :\npackage main\n\nimport \"fmt\"\n\ntype SQL struct {\n    table string\n}\n\nfunc (s SQL) Create () {\n    fmt.Println(\"Create\")\n}\nfunc (s SQL) Read () {\n    fmt.Println(\"Read\")\n}\nfunc (s SQL) Update (criteria string) {\n    fmt.Println(\"Update\", criteria)\n}\nfunc (s SQL) Delete (criteria string) {\n    fmt.Println(\"Delete\", criteria)\n}\n\ntype ORM interface{\n    Create()\n    Read()\n    Update(string)\n    Delete(string)\n}\n\nfunc main() {\n    sqldb := SQL{\"table\"}\n    var db ORM = sqldb\n    db.Create()\n    db.Read()\n    db.Update(\"record\")\n    db.Delete(\"table\")\n}\nBut you can notice that what if the operations were more than that ?\nWe will get bunch of SQL member method ! We want to write “Clean Code”.\nWe should follow the “Single-responsibility\nprinciple”(SRP) to design a more readable code. Struct SQL has deal\nwith too many things ! We can try to make Create as a\nsingle struct, Read as a single struct. So each struct just\ndo exactly one things. Let’s try the concept of inheritance\nInheritance in Golang :\npackage main\n\nimport \"fmt\"\n\ntype SQL struct {\n    table string\n}\n\ntype CreateSql struct {\n    SQL\n}\nfunc (c CreateSql) Operate () {\n    fmt.Println(\"Create\")\n}\n\ntype ReadSql struct {\n    SQL\n}\nfunc (s ReadSql) Operate () {\n    fmt.Println(\"Read\")\n}\n\ntype UpdateSql struct {\n    SQL\n}\nfunc (s UpdateSql) Operate (criteria string) {\n    fmt.Println(\"Update\", criteria)\n}\n\ntype DeleteSql struct {\n    SQL\n}\nfunc (s DeleteSql) Operate (criteria string) {\n    fmt.Println(\"Delete\", criteria)\n}\n\ntype ReadOnlyORM interface{\n    Operate()\n}\ntype SideEffectORM interface{\n    Operate(string)\n}\n\nfunc main() {\n    dbCreate := CreateSql{SQL{\"table\"}}\n    dbRead := ReadSql{SQL{\"table\"}}\n    dbUpdate := UpdateSql{SQL{\"table\"}}\n    dbDelete := DeleteSql{SQL{\"table\"}}\n\n    var read_only_orm ReadOnlyORM\n    read_only_orm = dbCreate\n    read_only_orm.Operate()\n    read_only_orm = dbRead\n    read_only_orm.Operate()\n\n    var side_effect_orm SideEffectORM\n    side_effect_orm = dbUpdate\n    side_effect_orm.Operate(\"test\")\n    side_effect_orm = dbDelete\n    side_effect_orm.Operate(\"table\")\n}\nNow we make each struct just do only one thing. If Update operation\nhas Bug, we can just modified UpdateSql struct without affecting other\nmethod.\nBut it seems like bringing owls to Athens for such little program.\nMaybe it is a good method, when the code is larger, not just 60\nlines.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-03-25T08:55:57+08:00",
    "input_file": {}
  },
  {
    "path": "posts/go-struct-interface/",
    "title": "Go Struct and Interface",
    "description": "Struct wrap things, Interface make a member-required function list",
    "author": [
      {
        "name": "Bo-Wei Chen",
        "url": "https://BWbwchen.github.io/"
      }
    ],
    "date": "2021-08-07",
    "categories": [
      "golang"
    ],
    "contents": "\nWe can think of struct as a wrapper of object. You can also check Golang spec\n(struct).\ntype TestStruct struct {\n    Interger            int\n    String              string\n    Bool                bool\n    SomeWeirdInterface  interface{}\n    OtherStruct         // we call it embedded field\n}\ntype OtherStruct struct {\n    Hello string\n}\nWe use Interface to group a set of method. You can think it as a list\nthat all structs that have implemented the required method. If you\nmissing one of the method, you can not in that interface.\nYou can also check Golang spec\n(interface).\ntype Human struct {\n    name string\n}\n\ntype Student struct {\n    Human\n    job string\n}\n\ntype Worker struct {\n    Human\n    job string\n    salary int \n}\n\nfunc (h Human) Info() {\n    fmt.Printf(\"I am %s\\n\", h.name)\n}\n\nfunc (s Student) Info () {\n    fmt.Printf(\"I am %s. I am a %s.\\n\", s.name, s.job)\n}\nfunc (w Worker) Info () {\n    fmt.Printf(\"I am %s. I am a %s. I have %d salary\\n\", w.name, w.job, w.salary)\n}\n\ntype Person interface {\n    Info()\n}\nwith main function :\nfunc main() {\n    Tom := Student{Human{\"Tom\"}, \"student\"}\n    Ben := Worker{Human{\"Ben\"}, \"worker\", 1000}\n    Susan := Human{\"Susan\"}\n\n    Tom.Info()\n    Ben.Info()\n    Susan.Info()\n\n    var i Person\n\n    // Becaus Student and Worker and Human all have implemented Info method\n    i = Tom\n    i.Info()\n\n    i = Ben\n    i.Info()\n\n    i = Susan\n    i.Info()\n}\nWe can see that Student, Worker, Human all have implemented the Info\nmethod, so the variabe i with type person interface will work well.\nI am Tom. I am a student.\nI am Ben. I am a worker. I have 1000 salary\nI am Susan\nI am Tom. I am a student.\nI am Ben. I am a worker. I have 1000 salary\nI am Susan\nBut what if I change the Human method to Hi() ?\ncannot use Susan (type Human) as type Person in assignment:\n        Human does not implement Person (missing Info method)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-03-25T08:55:57+08:00",
    "input_file": {}
  }
]
