[
  {
    "path": "posts/dmro-task-offloading-framework-for-edge-cloud-computing/",
    "title": "DMRO Task Offloading framework for Edge-Cloud Computing",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Bo-Wei Chen",
        "url": "https://BWbwchen.github.io/"
      }
    ],
    "date": "2022-06-05",
    "categories": [
      "distributed system",
      "edge computing"
    ],
    "contents": "\n\nContents\nTerminology\nProblem\nProblem formulation\nSystem Model\nIoT device’s workflow\nDecide where to execute\nthe task\nDelay (ignore the\ndecision makeing time)\nEnergy\nconsumption (ignore task transmission)\n\nProblem\n\nMethod\nInner Model\nLocal\nobjective function (reward of each parallel \\(s\\) DNNs output actions)\nFind the\nbest action from \\(s\\) parallel DNN\noutput\nReward\n\nOuter Model\n\nRelated work\nTraditional\nIntelligent with AI\n\n\n\nTerminology\nWhat is task offloading?\nEdge offloading migrates complex tasks from IoT devices to edge-cloud\nservers and it can break through the resource limitation of IoT devices.\nApplication Partition:\nDivide the task into a workflow with multiple associated subtasks and\noffload the subtasks separately.\nResource Allocation\nDecision problems is a NP-hard problem.\n\nWhat is edge server? cloud server? IoT devices?\nCloud server:\nProviding flexible and on-demand computing resources for IoT\ndevices.\nEdge server:\nDeciding which computing tasks need to be offloaded and providing a\nlimited amount of computing resources.\n\nimgimg\nProblem\nThere are lots of factors that will effect the task offloading\nalgorithm, e.g., failure when environment changes, slow learning speed\nof ML-based method.\nUse Meta-learning to adjust the model for different\nenvironment.\nUse DRL to make the task offloading decision.\n\nProblem formulation\n\nSystem Model\n\nIoT device’s workflow\n\\[\n    R_x = \\{v_1, e_{1, 2}, v_2, e_{2, 3}, \\cdots\\}\n\\]\\(v_i\\) is the task \\(i\\), and \\(e_{i,\nj}\\) is the data transmission between task \\(v_i\\) and \\(v_j\\).\n\nDecide where to execute the\ntask\n\\[\nb_{x, i} \\in b_0, b_1, b_2\n\\]\\(b_0 = [1, 0, 0]^T\\), \\(b_1 = [0, 1, 0]^T\\) and \\(b_2 = [0, 0, 1]^T\\).\n\nDelay (ignore the\ndecision makeing time)\nComputational delay\n\\[\n      T_i^c = \\{\\begin{array}{lr}\n      \\frac{v_i}{C_0}, & b_{x, i} = b_0\\\\\n      \\frac{v_i}{C_1}, & b_{x, i} = b_1\\\\\n      \\frac{v_i}{C_2}, & b_{x, i} = b_2\n      \\end{array}\n\\]\\(C_i\\) is the computing power of\ncorresponding offload method(frequency, Hz).\nTransmission delay\n\\[\n      T_{i, j}^t = \\{\\begin{array}{lr}\n      0, & \\text{At the same devices}\\\\\n      \\frac{e_{i, j}}{B_{0, 1}}, & \\text{a devices is at IoT, the\nother is at edge}\\\\\n      \\frac{e_{i, j}}{B_{1, 2}}, & \\text{a devices is at edge, the\nother is at cloud}\\\\\n      \\frac{e_{i, j}}{B_{0, 2}}, & \\text{a devices is at IoT, the\nother is at cloud}\n      \\end{array}\n\\]\\(B_{i, j}\\) is the bandwidth between\noffload devices \\(i\\) and \\(j\\).\nTotal delay\n\\[\nT_x = \\sum_{i = 1}^N(T_i^c + T_{i, i+1}^t)\n\\]\n\nEnergy consumption\n(ignore task transmission)\nA energy consumption is:\\[\nE = E_{IoT} + \\alpha E_{edge} + \\beta E_{cloud}\n\\]\nfor a IoT device \\(x\\), it’s energy\nconsumption is:\\[\nE_x = \\sum_{i=1}^N [E_{i, IoT}, E_{i, edge}, E_{i, cloud}]b_{x, i}\n\\]\n\nProblem\n\\[\n\\min_b Q(x, b) = \\sum_{x=1}^M (T_x + \\delta E_x)\n\\]\n\nMethod\nimg\nInner Model\n\nThis is a live training method. Once we have the input of workflow,\nenvironment and meta initial parameter, we train the model and find the\nbest decision. We use parallel \\(s\\) RL\nDNNs to generate the next action respected to the input states and then\ncalculate the rewards then store the state, actions and reward into\nmemory. If we add enough data into memory(add 5+ datas), we sample a\nstate vector and use it to train our RL DNN model. Once this whole\nprocess have iterate over a while, we use this DNN model to make the\ndecision.\n\nLocal\nobjective function (reward of each parallel \\(s\\) DNNs output actions)\n\\[\nF(S_i, a) = T_i^c + T_{i, i+1}^t+\\delta E_i\n\\]\nThe delay of this moment plus the energy consumption.\n\nFind the best\naction from \\(s\\) parallel DNN\noutput\nThe optimal solution is the action with the lowest \\(F\\).\n\nReward\nIf the action is the optimal solution, the reward is the negative\nvalue of the minimum \\(F\\).\nIf the action is NOT the optimal solution, the reward is the negative\nvalue of the maximum \\(F\\).\n\nOuter Model\nWe random sample a environment from the memory pool, and calculate\nthe action, reward and the next states, then store the state, actions\nand reward into memory pool. If we add enough data into memory(add 5+\ndatas), we sample a state vector and use it to train our RL DNN model.\nOnce this whole process have iterate over a while, we use this DNN model\nparameter as the meta parameter for inner model.\n\nRelated work\n\nTraditional\nThe method obtain results after multiple iterations and often involve\ntoo many complex calculation operations:\nMarkov processes\nqueue models\nHeuristic algorithms, but it need large amount of computation, which\nresults in high runtime cost:\nCOM\ngenetic algorithm by Goudarzi.\n\nIntelligent with AI\nUse DL to classify the final offloading position of the input task\ninformation.\nParallel and distributed DNNs.\nUse RL, but slow training\nActor-Critic\nDQN\n\n\n\n",
    "preview": "https://cdn.pixabay.com/photo/2017/01/18/10/01/cloud-computing-1989339_960_720.png",
    "last_modified": "2022-06-05T10:51:10+08:00",
    "input_file": {}
  },
  {
    "path": "posts/qemu/",
    "title": "QEMU",
    "description": "QEMU emulator notes",
    "author": [
      {
        "name": "Bo-Wei Chen",
        "url": "https://BWbwchen.github.io/"
      }
    ],
    "date": "2022-04-22",
    "categories": [
      "VM"
    ],
    "contents": "\n\nContents\nAdd a virtual disk\nStart the QEMU VM\nSetup bridge network\nSetup NFS\nMonitor\nLive migration\nPerformance testing\n\nIt is a software emulator\nWhat about KVM? Xen?\nIt is a supported module to boost the speed of qemu.\nEnable by -enable-kvm\n\n\nAdd a virtual disk\n          qemu-img create -f qcow2 ubuntu.qcow2 10G\nqcow2\nSomething like loopback file system.\nIt will occupy the space in the disk, If it actually that big.\nStart the QEMU VM\nqemu 4.2\nmanual\n          sudo qemu-system-x86_64 -cpu host -enable-kvm \\\n          -m 1G -smp 1 \\\n          -drive if=virtio,format=qcow2,file=ubuntu.qcow2 \\\n          -boot d -cdrom ubuntu-20.04.4-live-server-amd64.iso \\\n          -vnc :0\nSetup bridge network\nI use ubuntu netplan, which will\ngenerate the corresponding config for specific backend network manger,\nsuch as systemd-networkd.\n          network:\n              version: 2\n              renderer: networkd\n              ethernets:\n                  enp1s0:\n                      dhcp4: no\n              bridges:\n                  br0:\n                      interfaces: [enp1s0]\n                      addresses: [192.168.124.76/16]\n                      gateway4: 192.168.0.2\n                      nameservers:\n                          addresses: [140.114.64.1, 8.8.8.8]\n                      dhcp4: no\nAnd launch the vm with flag -nic tap\nSetup NFS\nOn NFS server:\n            $ sudo apt install nfs-kernel-server\n            $ sudo mkdir /mnt/nfs # setup the sync directory\n            $ sudo chown nobody:nogroup /mnt/nfs # remove any restriction in the directory\n            $ sudo chmod 777 /mnt/nfs # give all permission to all the file in the directory\n            $ sudo vim /etc/exports # Permission for accessing the NFS server\n            # in /etc/exports\n            /mnt/nfs  <client ip>(rw,sync,no_subtree_check,no_root_squash)\n            # /mnt/nfs_share  192.168.43.0/24(rw,sync,no_subtree_check) # allowed an entire subnet to have access to the NFS.\n            $ sudo exportfs -arv # export the NFS\nOn NFS client:\n            $ sudo apt install nfs-common\n            $ sudo mkdir /mnt/nfs # setup the sync directory\n            $ sudo mount -t nfs <server ip>:/mnt/nfs /mnt/nfs # mount NFS\nMonitor\nAdd flag -monitor telnet:127.0.0.1:5500,server,nowait\nand you can use telnet to mange the vm.\nLive migration\nIt actually stop the VM for a while, but in very short time.\nWe will need to add Monitor flag to the source vm.\nLaunch the destination vm with migration flag\n-incoming tcp:0:4400\n          $ telnet localhost 5500\n          (qemu) migrate -d tcp:<destination ip>:4400\n          (qemu) info migrate\nPerformance testing\nSysbench (cpu, memory\nperformance)\ntutorial\n              sysbench cpu --cpu-max-prime=20000 --max-time=20\nIperf (network)\nOn the server side :\n                  iperf -s\nOn the client side:\n                  iperf -c <server ip> -t <testing time(s)>\n\n\n\n",
    "preview": {},
    "last_modified": "2022-04-22T09:11:40+08:00",
    "input_file": {}
  },
  {
    "path": "posts/simulate-annealing-algorithm/",
    "title": "Simulate Annealing algorithm",
    "description": "Introduction to simulate annealing algorithm",
    "author": [
      {
        "name": "Bo-Wei Chen",
        "url": "https://BWbwchen.github.io/"
      }
    ],
    "date": "2022-03-31",
    "categories": [
      "convex optimization"
    ],
    "contents": "\n\nContents\nIdea\nAlgorithm (Find the\nmaximum)\n\nFor function \\(h\\)\nGood at find a global optimum by jumping out of local optimum\nIdea\nHigher temperature will have higher momentum\nLower temperature will have lower momentum and lower potential\nenergy\nYou will jump with big gap at high temperature and little gap at low\ntemperature\nAlgorithm (Find the maximum)\n\\(x\\) is start point\n\\(T_1\\) is start temperature\nFor \\(i\\) in 1 to N (Number of\niteration) or \\(T_{max}\\) to \\(T_{min}\\)\nSample a \\(\\xi\\) from normal\ndistribution or other distribution\nThe proposal solution is \\(x' = x \\pm\n\\xi\\)\nDecide that whether we accept the proposal\n\\(\\Delta h = h(x') -\nh(x)\\)\nif the delta value is positive for maximize\naccept the good change\n\nTry to accept the bad change ( \\(\\Delta h\n< 0\\) )\nCalculate probability \\(p = exp(\\Delta h /\nT_i)\\)\n\\(p\\) is high when \\(T\\) is high\nbad \\(\\Delta h\\) (very negative)\nwill have low \\(p\\)\n\nIf \\(p > rand(0,1)\\)\naccept, \\(x = x'\\)\n\nelse \\(x = x\\)\n\n\nUpdate the \\(T\\) with your own\nmethod\n\\(T\\) - -\n\\(T = \\alpha T, 0 < \\alpha <\n1\\)\n\n\n",
    "preview": "https://cdn.pixabay.com/photo/2021/10/07/16/28/challenge-6689107_960_720.png",
    "last_modified": "2022-03-31T16:17:29+08:00",
    "input_file": {}
  },
  {
    "path": "posts/raft/",
    "title": "Raft ",
    "description": "Paper note of Raft consensus algorithm",
    "author": [
      {
        "name": "Bo-Wei Chen",
        "url": "https://BWbwchen.github.io/"
      }
    ],
    "date": "2022-01-23",
    "categories": [
      "distributed system",
      "raft"
    ],
    "contents": "\n\nContents\nGoal\n5 Property\nConsensus Algorithm\nRaft Basics\nLeader election\nLog replication\nSafety\nDynamic\nmember in the cluster (configuration change mechanism)\nLog Compaction (by\nsnapshot)\nMajority Vote\nWith Application\nProblem\n\nGoal\nSolve split-brain by majority vote.\nLeader election\nLog replication\nSafety\nThe stronger degree of coherency\n5 Property\nElection safety: Only 1 leader.\nLeader Append-Only : leader only append new entry in its log. (No\ndelete its log).\nLog Matching : identify log entry by (term number, log index).\nLeader Completeness: if a log entry is committed in a given term,\nthen it will appear in the leader’s log.\nState Machine Safety: If a machine has an applied log entry\nat index \\(t\\) , then no other server\nwill have a applied different entry at index \\(t\\) .\nConsensus Algorithm\nKeeping the replicated log in replicated state machine\nconsistent.\nOnce commands are properly replicated, each server’s state machine\nprocess them and make each server identical\nRaft Basics\nTerms (Time, clock) When does the term end? #raft_problem\nA follower receives no communication over a period of\ntime(election timeout), and it begins an election to choose a\nnew leader.\n\n\nRaft divides time into terms.\nEach term begins with an election.\nAct as a logical clock to distinguish which server is newer.\nUse term number to detect inconsistencies.\n\nState How to know the total number of the cluster? #raft_problem\nBy configuration.\n\n\nRPCs Calls\nRequestVote\nAppendEntries\nreplicate log\nheartbeat message(append 0 entries)\n\n\nLog (Term number, Log index) can represent an unique log entry.\nA log entry is considered committed if it is stored\non majority of the servers (safe for that entry to be applied\nto the state machine).\nLeader will decides when to apply the log entry command.\n#raft_problem\n\n\nLeader election\nA follower receives no communication over a period of\ntime(election timeout), and it begins an election to choose a\nnew leader.\nheartbeat time << election timeout << infinity\nRaft paper use 10ms ~ 500ms.\n\nincrease its current term numbers and\nchange the state to the candidate.\n\nThere are 3 cases:\nIt wins the election\nit receives votes from a majority of the servers, which with\nthe same term, in the cluster.\nHow to vote? Vote for whom? → first-come-first-serve.\nIt became a leader and sends heartbeat messages to\nall of the other servers.\n\nanother server is a leader already.\nThe leader’s term >= the candidate’s term → candidate\nbecame a follower.\nThe leader’s term < the candidate’s term → candidate\nrejects the leader and continues the election.\n\nno winner(eg. split vote)\nretry with a randomized election timeout.\n\n\nThere are some restriction for leader election:\nLeader election restriction\n\nLog replication\nLeader receive the client request → append the command to leader’s\nlog → issue AppendEntries to all followers → leader\napplied the log and return.\nWhat if follower crash? #raft_problem\nleader retries the AppendEntries RPC\nindefinitely.\n\n\nConsistency issue\nproperties : (2 entries in different logs have :)\nthe same (term number, log index) → same entry command\n\nthe same (term number, log index) → all of the preceding\nentries are the same (consistency check)\n\n\nIf the follower receive a log entry which doesn’t have any matched\n(term number, log index) with follower’s log, refuse to update, drop\nit.\nLeader crash will lead to inconsistencies.\nCommitting entries from previous terms\n\nSolved by overwriting the followers’ logs with leader’s log.\nfind the latest log entry where leader and follower agree → delete\nall the log entry after that entry → leader send the remain part.\nleader will maintain a nextIndex for each follower. If the\nnextIndex is different, the RPC failed. → leader decrease the\nnextIndex and try to match.\nnextIndex = index of the next new log entry.\n(prevLogIndex, prevLogTerm)\n\nInitialize nextindex with leader’s next new log entry index.\n\n\n\n\nSafety\nEach state machine should execute exactly same commands in the same\norder.\nLeader election\nrestriction\nLeader for any given term contains the entire previous term\ncommitted log.\nEnsure this property from the moment of election.\n\nWhen election, candidate request a RequestVote RPC.\nthe voter will compare the (term number, index number)\nthe candidate is older than voter → deny.\nthe candidate is more up-to-date than voter → vote\nit.\n\nHow to compare? #raft_problem\nCompare term first, new is better, then compare log index, new is\nbetter.\n\n\n\nCommitting entries\nfrom previous terms\nWhat if the leader crashes during committing an\nentry? What should new leader do? How to determine commitment?\nA log entry is considered committed if it is stored\non majority of the servers (safe for that entry to be applied\nto the state machine).\nOnly term 1 was committed. (c) shouldn’t happen. \nOnly try to commit NOW new entry to the replicas, once we\ndone this, all prior (un-)committed entry will be automatically\ncommitted.\n\nFollower and candidate crashes\nSolved by overwriting the followers’ logs with leader’s log.\n\n\nDynamic\nmember in the cluster (configuration change mechanism)\nEach server has different timing to apply the new configuration.\nCan’t directly change, caused it will have 2 leader in some\ncases.\n\nTwo-phase approach\noriginal configuration → old and new configuration (joint consensus)\n→ new configuration\nIn the joint consensus phase, old and new configuration work\ntogether to serve Raft service.\n\nProblem\nNew servers need a long time to initialize.\nNew server will be in non-voting state until it caught up with the\nrest of the cluster. (by leader’s snapshot Log Compaction (by snapshot)\n)\n\nThe leader of joint consensus phase leader may not be part of the\nnew configuration.\nleader step down to follower state. Wait for a new election.\n\nRemoved server may disrupt the cluster by re-election.\nserver disregard(ignore) RequestVote RPC during the minimum election\ntimeout of hearing from the leader.\n\n\nLog Compaction (by snapshot)\n\nMajority Vote\nodd number of servers.\nIf you have \\(2x + 1\\) servers,\nthen there can tolerant at most \\(x\\)\nbroken servers in order to run normally.\n\nWith Application\nRecord the client request\n          Start(request) -> (log index, term numbers)\nProblem\nDoes the failure really frequently happen? #raft_problem\nFailure may not happened so frequently\nBut slow follower will happen frequently.\nCould we mix the slow follower and failure up? #raft_problem\n\n\n",
    "preview": "https://cdn.pixabay.com/photo/2019/11/19/07/18/network-4636686_960_720.jpg",
    "last_modified": "2022-03-27T23:07:42+08:00",
    "input_file": {}
  },
  {
    "path": "posts/vmware-ft-paper-note/",
    "title": "VMware FT paper note",
    "description": "Paper note of VMware FT",
    "author": [
      {
        "name": "Bo-Wei Chen",
        "url": "https://BWbwchen.github.io/"
      }
    ],
    "date": "2021-12-21",
    "categories": [
      "distributed system",
      "VMware FT"
    ],
    "contents": "\n\nContents\nVMware FT\nGoal\nConstraint\nReplication\nState Transfer\nReplicated State\nMachine\n\nVMWare FT\nNon-Deterministic\nEvents\nOutput Rule\nTest-and-Set\n\nVMware FT\nGoal\nUse Replication to achieve Fault-tolerance.\nConstraint\nCannot fix the software bug.\nReplication\nThey all copy their own state, but the definition of the state is\ndifferent.\nState Transfer\nPrimary backup its own state(memory,\ndata), and send them to the backup server. If the primary\nis fail-stop, we can use the backup server instead.\nReplicated State Machine\nPrimary backup its own state(command), and\nsend them to the backup server. If the primary is fail-stop, we can use\nthe backup server instead.\nVMWare FT\nIt replica all the things, including RAM, register…etc. But GFS will\nonly replica application-level data, such as chunks.\nUse disk server as the local disk.\nUse log channel for backup server to sync the primary log event.\n(eg, sync generating random numbers, etc.)\nNon-Deterministic Events\nThe time of the interrupt.\nNormally, the input in this system is a network package, while the\nDMA of the network card receives a package, it will copy the content\ninto memory, and trigger an interrupt, which could differ in time.\nSolution: Bounce Buffer. When a package is received. VMM will stop\nthe primary, and copy the package content into the primary memory and\ntrigger an interrupt of the primary’s network card and then memorize the\nid of the now instruction. It does something to the backup server and\nmakes an interrupt at the id of the instruction of the primary interrupt\ninstruction.\n\nWeird instructions\neg. use system time.\n\nMulti-process(didn’t be mentioned in this paper, this paper is only\nfor single-core processor)\nIt’s unpredictable for the order of the instruction execution on\nmulti-process.\n\nOutput Rule\nThe output of the primary will be sent by a simulated network card,\nand the output of the backup server will be discarded.\nWhat if the network between primary and backup server crash and the\nprimary dead? The values in backup and primary are different.\nWhen the backup server receives the primary log(input) first, then\nsend output to the outside client.\nBottleneck here, since the primary need to sync and wait for the\nbackup server.\n(What if) input into primary, but output from backup?\n\nWhat if the response had been sent to the client, but primary crash.\nBut the original request hasn’t been executed by the backup server?\nWhen the service switch to the backup server, it will wait until the\nbackup server consumes all the buffered request and have the same state\nas the original primary then it will start to take over the duty.\nDuplicate output?\nNope, since the output package will have the same information as the\nprimary output package, it will be filtered out at the TCP level.\n\n\nTest-and-Set\nWhat if the network between the primary and backup servers was\nbroken, but the primary and backup servers were all healthy? They all\nthink that the other is dead so it needs to take over for the duty.\nCall the third party service(test-and-set service) to decide,\nwhether use the primary or backup server.\nWhenever we need to change the primary, we need to connect with the\ntest-and-set service first to decide whether we could switch or not.\nIt’s like a lock.\n\n",
    "preview": "https://cdn.pixabay.com/photo/2019/11/19/07/18/network-4636686_960_720.jpg",
    "last_modified": "2022-03-27T23:08:06+08:00",
    "input_file": {}
  },
  {
    "path": "posts/google-file-system-paper-note/",
    "title": "Google File System paper note",
    "description": "Paper note of Google File System",
    "author": [
      {
        "name": "Bo-Wei Chen",
        "url": "https://BWbwchen.github.io/"
      }
    ],
    "date": "2021-12-20",
    "categories": [
      "distributed system",
      "gfs"
    ],
    "contents": "\n\nContents\nGFS\nGoal\nConstraint\nArchitecture\nMaster Node\nChunk Server\n\nTech\n讀\n寫\nGarbage Collection\nError Handle\nData Consistent\n\nProblem\n\nGFS\nGoal\n全局通用的儲存系統\n高容量高速→ 需要 sharding\n自動修復\n大量順序讀取，record append\nConstraint\n在一個 data center.\n可以允許一點點的錯誤\nonly guarantees that the data is written at least once as an atomic\nunit. → If writing success, data must have been written at the same\noffset on all replicas of some chunk.\nArchitecture\n單一 master node, 多個 chunk server\nMaster Node\n管理文件與 chunk info (file → chunk IDs)\nUse read-write lock → concurrent mutation in the same\ndirectory.\n\n\n儲存：（要存在硬碟，不用存在硬碟）\nfile → Chunk handlers\nChunk handlers → Chunk Data\n每個 chunk 在哪個 chunk server 上\nChunk version number\n主 chunk 在哪個 chunk server （因為寫入必須要在主 chunk）\n主 chunk expiration time.\n\n\n使用 log 紀錄每一次的操作\nChunk Server\n儲存實際 data\n64MB 為一個 chunk 並依照 chunk id 作為 filename 存成一個檔案\nTech\n讀\ngfs_readclient 發出請求讀 file, offset\nmaster 得知該 file 的 chunk id list，透過 offset / 64 得到對應的\nchunk id\nmaster 將該 chunk id 所在的 chunk server id list 與 chunk\nid(handle) 傳回給 client.\n\nclient 從 chunk server id list(cache this!) 挑一個獲取\ndata\nclient 將 chunk handle(id), byte range 交給 chunk server\nchunk server 將對應的 chunk id file with byte range 讀出交給\nclient\n寫\ngfs_write.pngRecord Append\n(追加在文件最後面)\n只能對 主 chunk (Primary Chunk) 來寫入\nBut, What if the primary chunk doesn’t exist?\nfind the up-to-date chunk replicas. (By the version number!!) → (Not\nfound? Crash)\nThe other will be the secondary chunk\nNotify the primary and secondary servers of their roles and new\nversion numbers. [The data in the primary and secondary servers are\nup-to-date??] [primary will notify the offset so if it\nis not up-to-date, it will leave a hole.]\nGiving the expiration time for the primary. (prevent multiple\nprimary servers, split-brain!!)\nIncrease the chunk version number\n\nIf the master wants to change the primary server, it can wait until\nthe primary is expired. (Not communication needed!)\nIf the writing data is too large, break it down into multiple write\noperations.\nClient 提供 filename 並要求追加內容→ master\nMaster → the server which contains the primary chunk (Primary\nserver)\nThe client sends the data to the primary and secondary servers, and\nthe server will save them in a temporary place. After the primary and\nsecondary server receives the data → “Receive the data!” to Client\nThe primary will check whether need a new chunk.\nOptimization: Send the data to the nearest server, and propagate it\nto the other server.\n\nClient “Append record now!” → Primary write the data and notify the\nsecondary server to write data.\nthe secondary will return the status to the primary server, “yes” or\n“no”.\nAll “yes” → “Done” to Client.\nelse → “Fail” to Client. The client should start from 1.\nBut!! The chunk server will Not recover the original data!\n\n\nGarbage Collection\nRename the filename into a hidden name with a deletion\ntimestamp.\nDuring the master’s regular scan, it removes the file and\nmetadata.\nError Handle\nData Consistent\n不同 chunk server 的 data 可能會略有不同！\nThe primary will notify the offset so if it is not up-to-date, it\nwill leave a hole.\nThe client needs to tolerant the out-of-order data order.\n\nIf you want to make it a\n強一致性 :\nDetect the repeated request. whether this request is retry.\nPrimary - secondary should be a two-phase commit.\nProblem\nOut of RAM of the master node.\nToo many requests for a single master node.\n\n\n\n",
    "preview": "https://cdn.pixabay.com/photo/2019/11/19/07/18/network-4636686_960_720.jpg",
    "last_modified": "2022-03-27T23:09:20+08:00",
    "input_file": {}
  },
  {
    "path": "posts/api-gateway-for-microservice-with-kong/",
    "title": "API gateway for microservice with Kong",
    "description": "API gateway with Kong",
    "author": [
      {
        "name": "Bo-Wei Chen",
        "url": "https://BWbwchen.github.io/"
      }
    ],
    "date": "2021-08-27",
    "categories": [
      "microservice"
    ],
    "contents": "\n\nContents\nIntroduction\nService discovery\nCode\n\nIntroduction\nI have implemented a JWT microservice. But I need an API gateway to\nredirect the outside HTTP request to the inner network. We need a server\nto receive an HTTP request and then redirect to the corresponding\nmicroservice. How do we know the actual IP address in our network? One\nis to hard-code the IP address in the API gateway. You can also use\nservice discovery to register your microservice IP address. So you can\ndynamically start service or shutdown service. In this tutorial, I use\nconsul as my service discovery registry.\nService discovery\nLet’s talk about service discovery first. There are 2 types of\nservice discovery. One is client-side, another is server-side.\nClient-side means you get the actual IP address of a microservice from\nthe service discovery registry first, then use that IP address to access\nthe microservice. Server-side means you send a request to the service\ndiscovery registry, and it will help you to query the corresponding\nmicroservice then send back the microservice response to you.\nCode\nI use consul as service registry and Kong as API gateway and use\ndocker-compose to set up my docker containers. ###\ndocker-compose.yml\nversion: \"3.5\"\nservices:\n  jwt_service:\n    container_name: jwt_service\n    build :\n      context : ./jwt\n      target: deploy \n    environment:\n      SECRETKEY: asdf\n      localIP: 192.168.18.5\n      PORT: 8087\n      consul_url: consul:8500\n    networks:\n      net:\n        ipv4_address: 192.168.18.5\n    expose :\n      - 8087\n    depends_on:\n      - consul\n\n  consul :\n    container_name : consul\n    image: consul\n    networks:\n      net:\n        ipv4_address: 192.168.18.4\n    expose:\n      - 8500\n      - 8600/udp\n    ports:\n      - \"8700:8500\"\n\n  kong:\n    image: kong:latest\n    volumes: \n      - ./kong.yml:/usr/local/kong/declarative/kong.yml\n    environment:\n      - KONG_DATABASE=off\n      - KONG_DECLARATIVE_CONFIG=/usr/local/kong/declarative/kong.yml\n      - KONG_PROXY_ACCESS_LOG=/dev/stdout\n      - KONG_ADMIN_ACCESS_LOG=/dev/stdout\n      - KONG_PROXY_ERROR_LOG=/dev/stderr\n      - KONG_ADMIN_ERROR_LOG=/dev/stderr\n      - KONG_ADMIN_LISTEN=0.0.0.0:8001, 0.0.0.0:8444 ssl\n      - KONG_DNS_RESOLVER=192.168.18.4:8600\n    ports:\n      - \"8000:8000\"\n      - \"8443:8443\"\n      - \"127.0.0.1:8001:8001\"\n      - \"127.0.0.1:8444:8444\"\n    networks:\n      - net\n\nnetworks:\n  net:\n    driver: bridge\n    ipam:\n      config:\n        - subnet: 192.168.18.0/24\n          gateway: 192.168.18.1\nWe need to edit the config file for the Kong API gateway. I set the\nDNS resolver to the consul. I use the docker bridge network to assign a\nstatic IP directly to each microservice container. If we didn’t assign a\nstatic IP, we use the service name to represent the IP address of each\ncontainer. When we access that IP, we will get the actual IP of that\ncontainer from docker. You can think that we use docker as a DNS\nresolver. But we use consul as DNS resolver! So it won’t know what is\nthe actual IP address of the service name. Make sure to assign a static\nIP to microservice! This is how I make it. If you have another good\nmethod to do it, please tell me!\nkong.yml\n_format_version: \"2.1\"\n\n_transform: false\n\nservices:\n  - name: jwt-service\n    url: http://jwt_service.service.dc1.consul\n    routes:\n      - name: jwt-route\n        paths: \n          - /jwt\nWe set the URL to the service name that we register to the\nconsul.\nThen I can access my microservice with path /jwt !\nIf you have any problems, feel free to ask me!\nYou can find me here 👉 tim.chenbw@gmail.com\nSubscribe to my substack here! 👉 subscribe\nme on substack !\n\n\n\n",
    "preview": "https://cdn.pixabay.com/photo/2015/10/31/11/58/call-center-1015274_960_720.jpg",
    "last_modified": "2022-03-27T23:10:23+08:00",
    "input_file": {}
  },
  {
    "path": "posts/tinyurl/",
    "title": "Tinyurl",
    "description": "Build a Full-Stack Tinyurl service in golang using Gin, Redis, MongoDB and Zookeeper",
    "author": [
      {
        "name": "Bo-Wei Chen",
        "url": "https://BWbwchen.github.io/"
      }
    ],
    "date": "2021-08-16",
    "categories": [
      "golang",
      "docker"
    ],
    "contents": "\n\nContents\nTinyurl\nArchitecture\nDeploy\nCI/CD (both frontend and\nbackend)\nProblem\nFuture\n\n\nTinyurl\nRecently, I learned the system design and tried my best to implement\na full-stack tinyurl service. Throughout this journey, I also learned\nhow to build a CI/CD pipeline (you can check this posts) and also learned how to use\nkubernetes.\nArchitecture\n\n\nI built this service on my own server, used nginx to serve static vue\nfrontend.\nI design this system aim to have 100 URLs requents per second, So the\nshorten name will have 7 characters and generated by use MD5.\nFirst, each api server got a range of number from Zookeeper, and then\nuse that number to generate the MD5 value, this method can guarantee\nthat the hash code won’t collision.\nSecond, take the first 7 characters as short name, you need to check\nwhether this short name is existing in the database. If existed in\ndatabase, just shift a character.\nThird, store into database and redis cache.\nFor get long URL, find in redis cache first. if not found, find in\nthe database.\nDeploy\nI deployed my service on rancher kubernetes 2.4.16 \nCI/CD (both frontend and\nbackend)\nI managed my code in self-hosted gitlab with gitlab-ci.\nbackend\n\nfrontend\n\nProblem\nAlthough I had checked the short name collision, I still got a\nlot of collision. Then I found it is the api server’s problem. A client\nsend request to api server, server will create a thread to handle that\nrequest, and use the counter number to compute the hash value. What if 2\nthread use counter value simultaneously ? They will use same value to\ncompute short name, and that short name doesn’t in database and they\nwill be store in database and redis cache!\nSo I make a mutex lock on counter variable to fix this bug.\nVue environment variable disappear when deploy on kubernetes.\nWhen I build the docker image of Vue frontend, I can’t build it with\nbackend way. In my api server, I can modify environment variable when I\ndeploy the service on Kubernetes. I can input the environment variable\nin deployment yaml file. But when I do the same thing to Vue frontend\ndocker image. I can not pass the environment variable via deployment\nyaml. At the end, I found that I should input the environment variable\nwhen I build the docker image.\nFuture\nI want to try to import microservice to this project, or future\nproject.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-03-27T23:11:42+08:00",
    "input_file": {}
  },
  {
    "path": "posts/epoll-study/",
    "title": "Epoll study ",
    "description": "let's study about epoll, blocking and non-blocking",
    "author": [
      {
        "name": "Bo-Wei Chen",
        "url": "https://BWbwchen.github.io/"
      }
    ],
    "date": "2021-08-13",
    "categories": [
      "epoll",
      "tcp"
    ],
    "contents": "\n\nContents\nBlocking and\nNon-Blocking\nSynchronous and\nAsynchronous\nNon-Blocking I/O in C\nReadiness of\nDescriptors\nThe bowels of epoll\n\n\nBlocking and Non-Blocking\nWhen you send a request to server. Server will prepare some data for\nyou (I/O operations). When you wait for data preparation, your program :\n1. hang up the thread until the server finished the preparation and send\nresponse to you. (Blocking) 2. will get response\nimmediately, but you probably got an error, if server hasn’t finished\nthe data preparation. Your program need to polling the server and\nchecking error code of each request. But you are not blocked !\n(Non-Blocking)\nSynchronous and Asynchronous\nWhen you send a request to server. Server will prepare some data for\nyou (I/O operations). When you wait for data preparation, your program :\n1. hand up the thread until the server finished the preparation and send\ntreponse to you. (Synchronous) 2. will get response\nimmediately, but server will run your request in “backgroung”. When\nserver finished your request, it will notify you, or run the\ncallback function. (Asynchronous)\nNon-Blocking I/O in C\nNormally, you can’t read or write directly to disk files. They\nusually do it via the kernel buffer cache as a proxy. But if you want to\nread or write directly, use O_SYNC flag when opening\nthe disk file.\nYou can put any file descriptor in the nonblocking mode, just setting\nthe no-delay flag O_NONBLOCK, which is an I/O operating\nmode, when opening the file.\nReadiness of Descriptors\nWe called it “ready”. When a file descriptor can perform an I/O\noperation without blocking, such as the arrival of new input or sock\nconnection establishment etc. There are 2 ways to find out the readiness\nof descriptors.\nLevel\nTriggered (select, poll, epoll) (when the condition is met)\nAt any time, we “try to”(or poll for) perform an I/O operation on an\nnon-blocking descriptor. If the I/O operation blocks, the system call\nreturns an error. If the I/O operation ready, we can actually perform\nthe entire I/O operation or just do nothing.\nEdge\nTriggered (Asynchronous) (when the status changed)\nWhen the I/O operation is ready, it will notify the process or “PUSH”\nto the process. The process can attempt to perform the maximum operation\nit possibly can every time it get a descriptor readiness notification,\nor it will need to wait until next notification arrival.\nBut even with both 2 non-blocking methods, an extremely large read or\nwrite call has the potential to block.\nSelect\nlevel triggered mechanism.\nint select(\n    int nfds, \n    fd_set *readfds, \n    fd_set *writefds,\n    fd_set *exceptfds, \n    struct timeval *timeout\n);\nSelect monitors 3 independent sets of descriptors. 1. readfds, check\nif a read will not block 2. writedfs, check if a write will not block 3.\nexceptfds, monitored for exceptional conditions.\nPoll\nBut what if you want not just read, write or exceptional conditions ?\nUse poll ! we pass in a set of descriptors each marked event that it\nneeds to track.\nint poll(struct pollfd *fds, nfds_t nfds, int timeout);\nstruct pollfd {\n    int   fd;         /* file descriptor */\n    short events;     /* requested events */\n    short revents;    /* returned events */\n};\nEpoll (event poll)\nEpoll instance is a kernel data structure. It allows for a process to\nmonotor multiplex I/O on mltiple descriptors. You have 3 system call to\ncontrol epoll instance :\nepoll_create()\n#include <sys/epoll.h>\n\nint epoll_create(int size);\nThe size augrument, which is the number of descriptors, is ignored\nsince Linux 2.6.8.\n#include <sys/epoll.h>\n\nint epoll_create1(int flags);\nflags can only be either 0 or EPOLL_COLEXEC. 1. flags is 0, it is the\nsame as epoll_create(). 1. flags is EPOLL_COLEXEC, it will set\nclose-on-exec flag on the new file descriptor.\nepoll_ctl()\n#include <sys/epoll.h>\n\nint epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);\nIt preformed op on target fd in\nepfd with event ##### op 1.\nEPOLL_CTL_ADD 2. EPOLL_CTL_MOD 3. EPOLL_CTL_DEL ##### events\ntypedef union epoll_data {\n    void        *ptr;\n    int          fd;\n    uint32_t     u32;\n    uint64_t     u64;\n} epoll_data_t;\n\nstruct epoll_event {\n    uint32_t     events;      /* Epoll events */\n    epoll_data_t data;        /* User data variable */\n};\nevents is just a bitmask that indicates which events\nfd is being monitored for. For example:\nEPOLLIN     : The associated file is available for read operations.\nEPOLLOUT    : The associated file is available for write operations.\nIf you want to perform multiple events, just OR-ing them. ####\nepoll_wait()\n#include <sys/epoll.h>\nint epoll_wait(int epfd, struct epoll_event *eventlist,\n                int maxevents, int timeout);\nblocks until any descriptor being monitored become ready for I/O\nThe bowels of epoll\nIn process\nLet’s first learn how the descriptor works. I create a file\ndescriptor in process A, I will get a file descriptor. The system will\nstore the descriptor and it’s file pointer in a table in my process A.\nEvery entry in this table has 2 fields : 1. descriptor flags. 2. file\npointer, point to an underlying kernel open file table. indexed by\ndescriptor.\nIn kernel\nEach file pointer point to an entry in kernel open file table. each\nentry in kernel open file table has 3 fields : 1. file offset 2. status\nflags 3. inode pointer.\nIn filesystem\nThen each inode pointer point to an entry in filesystem inode table.\neach entry in filesystem inode table has some fields : 1. file type. 2.\nfile locks. 3. etc.\nHow epoll works\nprocess A call epoll_create() to create an epoll\ninstance. The system will create an entry in kernel open file table, and\nit point to an entry in inode table. secondly, process A call\nepoll_ctl() to add a file descriptor fd0 to\nthe epoll instance’s interset list. The system will point the\nfd0 in epoll instance to fd0 in kernel open\nfile table (NOTICE: the kernel open file table entry is shared by\nreferenced descriptor !!) #### example Consider the situation above,\nprocess A fork process B without close-on-exec, process B will have\ntable as same as process B. for example, process A have fd0 and fork\nprocess B, it have a descriptor fd1. But fd0 and fd1 point to the same\nunderlying kernel open file table entry.\nFork epoll\nSo same case, but the fd0 and fd1 in the above case is epoll\ndescriptor. process A call epoll_wait(), after a moment it\ngot notifications. But at the same time, process B will also got the\nnotifications !!!\nSo we can say once a file descriptor is registered by a process with\nthe epoll instance, it will continue getting notifications about events\non the descriptor even if it closes the descriptors as long as the\nunderlying open file descriptor is still referenced by at least one\ndescriptor.\nPerformance\nSelect and poll are O(N) (or O(number of descriptors being\nmonitored)), but epoll is O(1) (or O(number of events that have\noccurred)). Because every time the descriptor become ready, kernel will\nadd it into the ready list in epoll instance. Once process call\nepoll_wait(), just return the ready list.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-03-25T08:55:57+08:00",
    "input_file": {}
  },
  {
    "path": "posts/moscow-notation/",
    "title": "Moscow notation",
    "description": "What is MOSCOW notation?",
    "author": [
      {
        "name": "Bo-Wei Chen",
        "url": "https://BWbwchen.github.io/"
      }
    ],
    "date": "2021-08-12",
    "categories": [
      "management"
    ],
    "contents": "\n\nContents\nMotivation\nMeaning\nExample\n\nMotivation\nDuring the internship at Logitech, I refactor and optimize the\ninternal test tool. When I finished the coding, my manager ask me to\nwrite a detailed document to record the core logic in this test\ntool.\nWhen I wrote the document, my manager ask me to use\nMOSCOW notation to list down the requirements of this\ntest tool.\nBut, What is MOSCOW notation ? 🤔\nMeaning\nMoSCoW is an acronym derived from four letters of each prioritization\n: M for MUST HAVE, S for SHOULD HAVE, C for COULD HAVE and W for WON’T\nHAVE. It didn’t have O, just for word pronounceable.\nExample\nSo when I write a test tool(prime detector) to detect prime number\nand composite number and we plan to detect floating point in the future,\nI will need to write the below requirement.\nPRIMEDETECTOR-1: MUST detect the prime number.\nPRIMEDETECTOR-2: MUST detect the composite number.\nPRIMEDETECTOR-3: SHOULD detect the floating point.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-03-25T08:55:57+08:00",
    "input_file": {}
  },
  {
    "path": "posts/cicd-on-gitlab-ci/",
    "title": "CI/CD on gitlab-ci",
    "description": "CI/CD on gitlab-ci with kubernetes",
    "author": [
      {
        "name": "Bo-Wei Chen",
        "url": "https://BWbwchen.github.io/"
      }
    ],
    "date": "2021-08-08",
    "categories": [
      "CI/CD",
      "docker",
      "docker-compose",
      "kubernetes"
    ],
    "contents": "\n\nContents\nEnvironment\ntinyURL\nTest on Local\nTest\nBuild\nDeploy\nFinal\n\nRecently, I have been writting a tinyURL service with a CI/CD\npipeline on self-hosted gitlab. I want to record this horrible and\ntime-consuming self-taught journey.\nOur CI/CD pipeline will be composed with test, build and deploy. We\nfirst test the code, check whether there is any mistake in our code\nlogic. Second, we build it to binary. Finally, we deploy our binary on\nthe cloud or your own kubernetes.\nEnvironment\ngolang 1.16.5\ndocker 20.10.7\ndocker-compose 1.29.2\nrancher 2.4.16\ngitlab-runner with docker executor\ntinyURL\ntinyURL was written for self-taught with backend developement,\nkubernetes, docker and DevOps. User will give the service a longURL.\nService will give back the short name of that URL. Once user query the\nservice with the short name, service will need to give back the original\nlong URL. for more info, check this\nThis is the overview architecture: \nTest on Local\nNotice that the whole system comprises 4 main module: api server,\nredis service, mongodb service and Zookeeper service. So I choose docker-compose\nto build a testing environment. Let’s check the setting yaml file:\nversion: \"3\"\nservices:\n    server:\n      container_name: myshorturl\n      image: shorturl_test\n      ports:\n        - 8081:8081\n      environment:\n        REDIS_URL: redis:6379\n        DB_URL: db:27017\n        ZOOKEEPER_URL: zookeeper:2181\n        PORT: 8081\n      depends_on:\n        - redis\n        - db\n        - zookeeper\n    redis:\n      container_name: redis\n      image: redis:alpine\n      expose:\n        - 6379\n    db:\n      container_name: monogodb\n      image: mongo:4.0.26-xenial\n      expose:\n        - 27017\n    zookeeper:\n      container_name: zookeeper\n      image: zookeeper:3.7.0\n      expose:\n        - 2181\nThere are 3 things to notice: 1. Firstly, Make sure the version\nnumber is 3, or it will fail in the testing pipeline. 2. Secondly, if\napi server want to connect with other service, connect the service name\ndirectly. eg: How to connect db ? just\n<dbprotocol>://db, such like : postgresql://db 3.\nLastly, the difference between ports and expose. - Ports will expose the\nport number and also publish them to the host machine.\n- Expose will expose the port between the containers,\nit will not publish the port to the host machine.\nThen run with command (it will shutdown containers automatically, if\napi-server finished test):\n$ docker-compose up --abort-on-container-exit --exit-code-from server\nTest\ntest-job:\n  stage: test\n  before_script:\n    - apk add docker-compose\n    - docker build -f Dockerfile-ci --tag shorturl_test .\n  script:\n    - docker-compose up --abort-on-container-exit --exit-code-from server\nSince we use gitlab-runner docker executor with\ndocker:stable image as default image, it only\nprovide docker command. You can check the official\ndocumentation. We need to install the docker-compose command for\ntesting. use apk to install it. Then build a docker image for golang\ntesting command.\n# Dockerfile-ci\nFROM golang:1.16\nWORKDIR /app\nCOPY . .\n\nCMD [ \"go\", \"test\", \"-v\" \nBuild\nbuild-job:  \n  image: \n    name: gcr.io/kaniko-project/executor:debug\n    entrypoint: [\"\"]\n  stage: build\n  script:\n     - mkdir -p /kaniko/.docker\n     - echo \"{\\\"auths\\\":{\\\"$DOCKER_REGISTRY\\\":{\\\"username\\\":\\\"${DOCKER_REGISTRY_USER}\\\",\\\"password\\\":\\\"${DOCKER_REGISTRY_PASSWORD}\\\"}}}\" > /kaniko/.docker/config.json\n     - /kaniko/executor --context $CI_PROJECT_DIR --dockerfile $CI_PROJECT_DIR/Dockerfile --destination $DOCKER_IMAGE\nWe use pre-build image to handle this stage.\nDeploy\nNow, We will try to deploy our service on kubernetes. I use rancher\nfor kubernetes.\nThere are some things need to sure. 1. kubeconfig file. 2. docker\nregistry username, password\nFirst, deploy a deployment on kubernetes. So we can automatically\ndeploy the next job trigger.\ndeploy-job:\n  stage: deploy\n  image: dtzar/helm-kubectl\n  before_script:\n    - sed -ie \"s/deploy-date-value/$(date)/g\" kubernetes/deploy.yaml\n    - mkdir -p /root/.kube/ && touch /root/.kube/config\n    - echo ${KUBERNETES_KUBE_CONFIG} | base64 -d > ${KUBECONFIG}\n  script:\n    - kubectl apply -f kubernetes/deploy.yaml\nI use deply-date-value as a trigger to trigger the deployment\nupdate.\nFinal\nThe CI/CD pipeline should run successfully ! Hooray ! 🎉\nIf you have other problem, feel free to ask me !\nYou can find me here 👉 tim.chenbw@gmail.com\n\n\n\n",
    "preview": {},
    "last_modified": "2022-03-25T08:55:57+08:00",
    "input_file": {}
  },
  {
    "path": "posts/go-interface-for-inheritance/",
    "title": "Go interface for inheritance",
    "description": "CRUD with clean code by using Go embed struct",
    "author": [
      {
        "name": "Bo-Wei Chen",
        "url": "https://BWbwchen.github.io/"
      }
    ],
    "date": "2021-08-07",
    "categories": [
      "golang"
    ],
    "contents": "\nLet’s check this example :\ntype SQL struct {\n    table SQL.table\n}\nWe want CRUD (create, read, update, delete) operation. So you might\nwrite the code like this :\npackage main\n\nimport \"fmt\"\n\ntype SQL struct {\n    table string\n}\n\nfunc (s SQL) Create () {\n    fmt.Println(\"Create\")\n}\nfunc (s SQL) Read () {\n    fmt.Println(\"Read\")\n}\nfunc (s SQL) Update (criteria string) {\n    fmt.Println(\"Update\", criteria)\n}\nfunc (s SQL) Delete (criteria string) {\n    fmt.Println(\"Delete\", criteria)\n}\n\ntype ORM interface{\n    Create()\n    Read()\n    Update(string)\n    Delete(string)\n}\n\nfunc main() {\n    sqldb := SQL{\"table\"}\n    var db ORM = sqldb\n    db.Create()\n    db.Read()\n    db.Update(\"record\")\n    db.Delete(\"table\")\n}\nBut you can notice that what if the operations were more than that ?\nWe will get bunch of SQL member method ! We want to write “Clean Code”.\nWe should follow the “Single-responsibility\nprinciple”(SRP) to design a more readable code. Struct SQL has deal\nwith too many things ! We can try to make Create as a\nsingle struct, Read as a single struct. So each struct just\ndo exactly one things. Let’s try the concept of inheritance\nInheritance in Golang :\npackage main\n\nimport \"fmt\"\n\ntype SQL struct {\n    table string\n}\n\ntype CreateSql struct {\n    SQL\n}\nfunc (c CreateSql) Operate () {\n    fmt.Println(\"Create\")\n}\n\ntype ReadSql struct {\n    SQL\n}\nfunc (s ReadSql) Operate () {\n    fmt.Println(\"Read\")\n}\n\ntype UpdateSql struct {\n    SQL\n}\nfunc (s UpdateSql) Operate (criteria string) {\n    fmt.Println(\"Update\", criteria)\n}\n\ntype DeleteSql struct {\n    SQL\n}\nfunc (s DeleteSql) Operate (criteria string) {\n    fmt.Println(\"Delete\", criteria)\n}\n\ntype ReadOnlyORM interface{\n    Operate()\n}\ntype SideEffectORM interface{\n    Operate(string)\n}\n\nfunc main() {\n    dbCreate := CreateSql{SQL{\"table\"}}\n    dbRead := ReadSql{SQL{\"table\"}}\n    dbUpdate := UpdateSql{SQL{\"table\"}}\n    dbDelete := DeleteSql{SQL{\"table\"}}\n\n    var read_only_orm ReadOnlyORM\n    read_only_orm = dbCreate\n    read_only_orm.Operate()\n    read_only_orm = dbRead\n    read_only_orm.Operate()\n\n    var side_effect_orm SideEffectORM\n    side_effect_orm = dbUpdate\n    side_effect_orm.Operate(\"test\")\n    side_effect_orm = dbDelete\n    side_effect_orm.Operate(\"table\")\n}\nNow we make each struct just do only one thing. If Update operation\nhas Bug, we can just modified UpdateSql struct without affecting other\nmethod.\nBut it seems like bringing owls to Athens for such little program.\nMaybe it is a good method, when the code is larger, not just 60\nlines.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-03-25T08:55:57+08:00",
    "input_file": {}
  },
  {
    "path": "posts/go-struct-interface/",
    "title": "Go Struct and Interface",
    "description": "Struct wrap things, Interface make a member-required function list",
    "author": [
      {
        "name": "Bo-Wei Chen",
        "url": "https://BWbwchen.github.io/"
      }
    ],
    "date": "2021-08-07",
    "categories": [
      "golang"
    ],
    "contents": "\nWe can think of struct as a wrapper of object. You can also check Golang spec\n(struct).\ntype TestStruct struct {\n    Interger            int\n    String              string\n    Bool                bool\n    SomeWeirdInterface  interface{}\n    OtherStruct         // we call it embedded field\n}\ntype OtherStruct struct {\n    Hello string\n}\nWe use Interface to group a set of method. You can think it as a list\nthat all structs that have implemented the required method. If you\nmissing one of the method, you can not in that interface.\nYou can also check Golang spec\n(interface).\ntype Human struct {\n    name string\n}\n\ntype Student struct {\n    Human\n    job string\n}\n\ntype Worker struct {\n    Human\n    job string\n    salary int \n}\n\nfunc (h Human) Info() {\n    fmt.Printf(\"I am %s\\n\", h.name)\n}\n\nfunc (s Student) Info () {\n    fmt.Printf(\"I am %s. I am a %s.\\n\", s.name, s.job)\n}\nfunc (w Worker) Info () {\n    fmt.Printf(\"I am %s. I am a %s. I have %d salary\\n\", w.name, w.job, w.salary)\n}\n\ntype Person interface {\n    Info()\n}\nwith main function :\nfunc main() {\n    Tom := Student{Human{\"Tom\"}, \"student\"}\n    Ben := Worker{Human{\"Ben\"}, \"worker\", 1000}\n    Susan := Human{\"Susan\"}\n\n    Tom.Info()\n    Ben.Info()\n    Susan.Info()\n\n    var i Person\n\n    // Becaus Student and Worker and Human all have implemented Info method\n    i = Tom\n    i.Info()\n\n    i = Ben\n    i.Info()\n\n    i = Susan\n    i.Info()\n}\nWe can see that Student, Worker, Human all have implemented the Info\nmethod, so the variabe i with type person interface will work well.\nI am Tom. I am a student.\nI am Ben. I am a worker. I have 1000 salary\nI am Susan\nI am Tom. I am a student.\nI am Ben. I am a worker. I have 1000 salary\nI am Susan\nBut what if I change the Human method to Hi() ?\ncannot use Susan (type Human) as type Person in assignment:\n        Human does not implement Person (missing Info method)\n\n\n\n",
    "preview": {},
    "last_modified": "2022-03-25T08:55:57+08:00",
    "input_file": {}
  }
]
